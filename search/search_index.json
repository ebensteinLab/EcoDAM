{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the EcoDAM Analysis Library This library provides tools to process data related to the EcoDAM project by TAU's Ebenstein lab, led by Gil Nifker. The library is an assortments of tools, GUIs, functions and objects that together provide a somewhat cohesive environment to work on the scientific questions that are related to this project. As it is work in progress, some parts of this library might not be as well-documented and tested as others, so please take that into consideration when using the different functions provided here. Most of the analysis in this project is done on BedGraph files, which results in a heavy focus on this filetype in this environment. The library can read, write, filter and smooth BedGraphs using a variety of built-in methods and functions. Some tools are more focused toward non-coders - i.e. they're (well documented) GUIs that provide a specific functionality, like smoothing, to their users. Other parts of the library are plane functions and classes which should be used to construct analysis pipelines on BedGraph files. Lastly, a few such pipelines already exist in the library, usually in the form of a Jupyter Notebook. These pipelines are work-in-progress, and are prone for errors and bugs. However, they do provide a hands-on example on how to use the provided functions in this library with real-world data to answer real-world questions. These docs contain a few examples regarding the main data types and structures of this library, and also contain an extensive API reference section.","title":"Home"},{"location":"#welcome-to-the-ecodam-analysis-library","text":"This library provides tools to process data related to the EcoDAM project by TAU's Ebenstein lab, led by Gil Nifker. The library is an assortments of tools, GUIs, functions and objects that together provide a somewhat cohesive environment to work on the scientific questions that are related to this project. As it is work in progress, some parts of this library might not be as well-documented and tested as others, so please take that into consideration when using the different functions provided here. Most of the analysis in this project is done on BedGraph files, which results in a heavy focus on this filetype in this environment. The library can read, write, filter and smooth BedGraphs using a variety of built-in methods and functions. Some tools are more focused toward non-coders - i.e. they're (well documented) GUIs that provide a specific functionality, like smoothing, to their users. Other parts of the library are plane functions and classes which should be used to construct analysis pipelines on BedGraph files. Lastly, a few such pipelines already exist in the library, usually in the form of a Jupyter Notebook. These pipelines are work-in-progress, and are prone for errors and bugs. However, they do provide a hands-on example on how to use the provided functions in this library with real-world data to answer real-world questions. These docs contain a few examples regarding the main data types and structures of this library, and also contain an extensive API reference section.","title":"Welcome to the EcoDAM Analysis Library"},{"location":"BedGraphAccessor/","text":"The BedGraphAccessor DataFrame Accessor Perhaps the most useful abstraction that this library contains is the BedGraphAccessor class, which generates a new accessor for existing pandas DataFrames by simply importing that class. This unique feature lets us utilize the full power of DataFrame, which are a great in-memory model for a BedGraph file, while plugging in to them additional functionality that can aid in doing BedGraph specific tasks. A core principle with this accessor is the two different states it lets the DF be in. A DF with a .bg accessor can either be in an 'index' mode or in a 'columns' mode. This state isn't kept anywhere, but methods that work with the BedGraph DF should be explicit in requiring a specific state for the data. The way to transition between these states is using the index_to_columns and columns_to_index methods, which largely work exactly as they say. As of now, largely due to time limitations, there's not much functionality baked directly into this accessor. That's not too bad, because one has to be careful to not overcrowd this new namespace, and sometimes it's better for some functions to rely in a different scope. Regardless, the most interesting methods are easily the weighted_overlap and unweighted_overlap pair.","title":"BedGraphAccessor"},{"location":"BedGraphAccessor/#the-bedgraphaccessor-dataframe-accessor","text":"Perhaps the most useful abstraction that this library contains is the BedGraphAccessor class, which generates a new accessor for existing pandas DataFrames by simply importing that class. This unique feature lets us utilize the full power of DataFrame, which are a great in-memory model for a BedGraph file, while plugging in to them additional functionality that can aid in doing BedGraph specific tasks. A core principle with this accessor is the two different states it lets the DF be in. A DF with a .bg accessor can either be in an 'index' mode or in a 'columns' mode. This state isn't kept anywhere, but methods that work with the BedGraph DF should be explicit in requiring a specific state for the data. The way to transition between these states is using the index_to_columns and columns_to_index methods, which largely work exactly as they say. As of now, largely due to time limitations, there's not much functionality baked directly into this accessor. That's not too bad, because one has to be careful to not overcrowd this new namespace, and sometimes it's better for some functions to rely in a different scope. Regardless, the most interesting methods are easily the weighted_overlap and unweighted_overlap pair.","title":"The BedGraphAccessor DataFrame Accessor"},{"location":"about/","text":"About this library This library was originally developed by Hagai Har-Gil for a project in TAU's NanoBioPhotonix lab of Prof. Yuval Ebenstein, to assist Gil Nifker with her Ph.D. work. The source code of the project is available here under the MIT license. The documentation was built using MkDocss .","title":"About"},{"location":"about/#about-this-library","text":"This library was originally developed by Hagai Har-Gil for a project in TAU's NanoBioPhotonix lab of Prof. Yuval Ebenstein, to assist Gil Nifker with her Ph.D. work. The source code of the project is available here under the MIT license. The documentation was built using MkDocss .","title":"About this library"},{"location":"notebooks/","text":"Data Analysis Notebooks Much of the actual work in the library was done in notebooks. Since this was pretty much an exploratory project, and largely still is, these notebooks contain many strictly wrong ideas and some partly misleading ones as well, obviously not due to malicious intent. The earlier notebooks call many functions that were either deleted, edited or deprectated along the way. Thus it's best to not use the output of these notebook, only the ideas they show and explored should be reviewed. In other words, there's no guarantee that the code they contain runs, and even if it runs it might be doing something else than it used to when it was first written down. The first notebook was exploration_eco_atac.ipynb and it should largely be ignored. The better parts of it were re-written and expanded upon in normalize_chrom_to_atac.ipynb , but this notebook is also somewhat outdated, although many of its figures are definitely of interest. These two notebooks have no real ordering principle - they shoot everywhere and try multiple things simulatneosly. In contrast, the next three notebooks are more focused on a specific task\\question. First is peak_calling.ipynb , which contains the exact recipes on how to do peak calling on ATAC data, and also contain some minimal post-processing on this data. If you need to know something about peak calling this is where you should probably look first. The next notebook is ml_based_classification.ipynb , which is an effort I made to find open chromatin areas using Machine Learning. It's not complete and I haven't got enough time to finish it, so this notebook can largely be ignored. If someone ever wishes to explore that angle they're more than welcome to contact Hagai. The last one is venn.ipynb that tries to generate a psuedo-Venn diagram out of all three methods - EcoDAM, ATAC and DNase. The role of this diagram is to measure the intersection of the three methods. We aleady saw that under some conditions the overlap between ATAC and DNase can be quite low, and we wanted to expand on that by introducing our own method into this diagram. Also, since this was the last notebook that I (Hagai) worked on, the called methods in this notebook are the most recent ones.","title":"Notebooks"},{"location":"notebooks/#data-analysis-notebooks","text":"Much of the actual work in the library was done in notebooks. Since this was pretty much an exploratory project, and largely still is, these notebooks contain many strictly wrong ideas and some partly misleading ones as well, obviously not due to malicious intent. The earlier notebooks call many functions that were either deleted, edited or deprectated along the way. Thus it's best to not use the output of these notebook, only the ideas they show and explored should be reviewed. In other words, there's no guarantee that the code they contain runs, and even if it runs it might be doing something else than it used to when it was first written down. The first notebook was exploration_eco_atac.ipynb and it should largely be ignored. The better parts of it were re-written and expanded upon in normalize_chrom_to_atac.ipynb , but this notebook is also somewhat outdated, although many of its figures are definitely of interest. These two notebooks have no real ordering principle - they shoot everywhere and try multiple things simulatneosly. In contrast, the next three notebooks are more focused on a specific task\\question. First is peak_calling.ipynb , which contains the exact recipes on how to do peak calling on ATAC data, and also contain some minimal post-processing on this data. If you need to know something about peak calling this is where you should probably look first. The next notebook is ml_based_classification.ipynb , which is an effort I made to find open chromatin areas using Machine Learning. It's not complete and I haven't got enough time to finish it, so this notebook can largely be ignored. If someone ever wishes to explore that angle they're more than welcome to contact Hagai. The last one is venn.ipynb that tries to generate a psuedo-Venn diagram out of all three methods - EcoDAM, ATAC and DNase. The role of this diagram is to measure the intersection of the three methods. We aleady saw that under some conditions the overlap between ATAC and DNase can be quite low, and we wanted to expand on that by introducing our own method into this diagram. Also, since this was the last notebook that I (Hagai) worked on, the called methods in this notebook are the most recent ones.","title":"Data Analysis Notebooks"},{"location":"tools/","text":"EcoDAM-Associated Tools A few recurring tasks in this project demanded the creation of specific tools that could perform these operations without any knowledge of this library and of Python at all. In general, these are GUI-based tools that are run as a standard application from a desktop, although they can be run as shell scripts. Both the source code of the app itself and of the runner shell script can be found in the tools folder of the repo. Below we'll list and describe the different tools and their purpose. This comes, of course, in addition to the built-in documentation of each of those applications. Histogram This tool's purpose is to draw a histogram of the supplied data. The data file it expects is a BedGraph, but as long as the data column you wish to plot is the last one (the rightmost) this tool should work for you. If you wish you can manually set the histogram's properties - its limits, number of bins, etc. In addition, from that histogram's figure you can also change some properties of the display, like use log scaling, and save that image to disk in whatever format you wish. Find Peaks A basic tool to detect peaks in a BedGraph, when full blown peak calling isn't necessary. The main peak detection parameters are user-defined and a resulting BedGraph pointing only at the peaks is written to disk with a _peaks suffix. Interactive Intensity Plot the values of many individual tracks at the same time and inspect the results interactively. This can be used to compare molecule brightness in different tracks, for example. Smooth BedGraph As the name implies, this tool can do smoothing-related operations on BedGraphs. Its set of features includes: Smooth a given BedGraph using some windowing function (i.e. Gaussian or boxcar) with a user-defined set of parameters. Coerce a given BedGraph to a new set of loci defined by a different BedGraph. Resample a BedGraph with a different 'jump size' between each entry. Do a combination of all of the above. Together these functions are quite powerful - for example this tool was used to smooth out the EcoDAM theoretical value, given at 1 bp resolution, so that it will be more similar to the way the imaging system outputs the data - 1kbp resolution and PSF-induced smearing. Convert Scientific Notation This Rust script iterates over the rows of the file and changes scientific notation (1.2e6) to regular notation (1200000). Compile and run it using rustc -O convert_bed_with_scientific_notation.rs && ./convert_bed_with_scientific_notation . Add 'chr' Prefix Another Rust script, this time to add a missing chr prefix for the given BedGraph, row-wise.","title":"Tools"},{"location":"tools/#ecodam-associated-tools","text":"A few recurring tasks in this project demanded the creation of specific tools that could perform these operations without any knowledge of this library and of Python at all. In general, these are GUI-based tools that are run as a standard application from a desktop, although they can be run as shell scripts. Both the source code of the app itself and of the runner shell script can be found in the tools folder of the repo. Below we'll list and describe the different tools and their purpose. This comes, of course, in addition to the built-in documentation of each of those applications.","title":"EcoDAM-Associated Tools"},{"location":"tools/#histogram","text":"This tool's purpose is to draw a histogram of the supplied data. The data file it expects is a BedGraph, but as long as the data column you wish to plot is the last one (the rightmost) this tool should work for you. If you wish you can manually set the histogram's properties - its limits, number of bins, etc. In addition, from that histogram's figure you can also change some properties of the display, like use log scaling, and save that image to disk in whatever format you wish.","title":"Histogram"},{"location":"tools/#find-peaks","text":"A basic tool to detect peaks in a BedGraph, when full blown peak calling isn't necessary. The main peak detection parameters are user-defined and a resulting BedGraph pointing only at the peaks is written to disk with a _peaks suffix.","title":"Find Peaks"},{"location":"tools/#interactive-intensity","text":"Plot the values of many individual tracks at the same time and inspect the results interactively. This can be used to compare molecule brightness in different tracks, for example.","title":"Interactive Intensity"},{"location":"tools/#smooth-bedgraph","text":"As the name implies, this tool can do smoothing-related operations on BedGraphs. Its set of features includes: Smooth a given BedGraph using some windowing function (i.e. Gaussian or boxcar) with a user-defined set of parameters. Coerce a given BedGraph to a new set of loci defined by a different BedGraph. Resample a BedGraph with a different 'jump size' between each entry. Do a combination of all of the above. Together these functions are quite powerful - for example this tool was used to smooth out the EcoDAM theoretical value, given at 1 bp resolution, so that it will be more similar to the way the imaging system outputs the data - 1kbp resolution and PSF-induced smearing.","title":"Smooth BedGraph"},{"location":"tools/#convert-scientific-notation","text":"This Rust script iterates over the rows of the file and changes scientific notation (1.2e6) to regular notation (1200000). Compile and run it using rustc -O convert_bed_with_scientific_notation.rs && ./convert_bed_with_scientific_notation .","title":"Convert Scientific Notation"},{"location":"tools/#add-chr-prefix","text":"Another Rust script, this time to add a missing chr prefix for the given BedGraph, row-wise.","title":"Add 'chr' Prefix"},{"location":"api/ecodam_py.bedgraph/","text":"module ecodam_py . bedgraph </> This module provides the basic building blocks for working with BedGraphs Attributes Equalized \u2014 A unique return type for equalize_loci Classes Equalized \u2014 Equalized(even, at_1bp, groups) </> BedGraphAccessor \u2014 Introduces a .bg accessor to DataFrame which provides unique capabilities for the DF, including new methods and properties. </> BedGraphFile \u2014 A BedGraphFile file which can be manipulated an displayed. </> Functions equalize_loci ( first , second ) (Equalized, Equalized) \u2014 Generate an approximately-equal loci DFs from the two given ones. </> intervals_to_1bp_mask ( start , end , orig_groups ) (ndarray, ndarray) \u2014 Generate a new 1bp BedGraph and keep information of the original distribution and sources of data. </> pad_with_zeros ( nfr , chrom ) \u2014 Adds zero entries for loci which are not included in one of the given DFs </> put_dfs_on_even_grounds ( dfs ) (iterable of DataFrame) \u2014 Asserts overlap of all given DataFrames. </> class ecodam_py.bedgraph . Equalized ( even , at_1bp , groups ) </> Bases tuple Equalized(even, at_1bp, groups) class ecodam_py.bedgraph . BedGraphAccessor ( pandas_obj ) </> Introduces a .bg accessor to DataFrame which provides unique capabilities for the DF, including new methods and properties. To use them, simply import this class to your current scope. Methods add_chr ( chr_ ) (pd.DataFrame) \u2014 Adds a 'chr' column to a copied DF. </> columns_to_index ( ) (pd.DataFrame) \u2014 Encodes the loci information in the index of the DF. </> index_to_columns ( ) (A modified DF with two new columns for the locus info) \u2014 Encodes the loci information in two columns of the DF. </> serialize ( fname , mode ) \u2014 Writes the BedGraph to disk. </> to_1bp_resolution ( multi_chrom ) (DataFrame) \u2014 Changes the coordinates of the given DF to have 1bp resolution. </> unweighted_overlap ( other ) (pd.DataFrame, pd.DataFrame) \u2014 Find the overlapping parts of self and other and return these areas. </> weighted_overlap ( other , overlap_pct ) (pd.DataFrame, pd.DataFrame) \u2014 Find the overlapping parts of the two DFs with at least 'overlap_pct' amount of overlap. </> method index_to_columns ( ) </> Encodes the loci information in two columns of the DF. This method takes a DF that has its locus information encoded in its index, possibly as a result of calling columns_to_index on it, and changes it so that its index becomes a standard RangeIndex and the loci infomration is kept in two np.int64 columns, 'start_locus' and 'end_locus'. method columns_to_index ( ) </> Encodes the loci information in the index of the DF. This method takes a DF that has its locus information as columns, i.e. 'start_locus' and 'end_locus', and moves these columns to a new pd.IntervalIndex instead. Returns (pd.DataFrame) A new DF with an IntervalIndex instead of the 'start_locus' and 'end_locus' columns method add_chr ( chr_='chr15' ) </> Adds a 'chr' column to a copied DF. Useful in certain internal situations mostly. Parameters chr (str, optional) \u2014 The chromosome name to add Returns (pd.DataFrame) A new DF with the new column method serialize ( fname , mode='w' ) </> Writes the BedGraph to disk. This method first normalizes the BedGraph and then writes it to disk with the given write mode. Parameters fname (pathlib.Path) \u2014 Filename to write to mode (str) \u2014 File writing mode (similar to the built-in open()). By default 'w' method unweighted_overlap ( other ) </> Find the overlapping parts of self and other and return these areas. This method looks for overlapping parts of the two BedGraph DFs and returns only the relevant slices of these two objects. The overlap is called 'unweighted' because it considers an area to be overlapping even if a single BP overlaps between the two loci. This might seem odd, but it's helpful since we usually work with BedGraph files that have the same coordinates, so this method is good enough. Use the (slower) 'weighted_overlap' method if you need to assert that the overlap is signifanct in terms of BP counts. Parameters other (pd.DataFrame) \u2014 A BedGraph DF Returns (pd.DataFrame, pd.DataFrame) The 'self' and 'other' DFs only at the rows that overlap method weighted_overlap ( other , overlap_pct=0.75 ) </> Find the overlapping parts of the two DFs with at least 'overlap_pct' amount of overlap. In the first phase of this method, the two DFs are put on the same coordinates so that they could be compared in a viable manner. Currently the way its done is to move them to 1bp resolution which automatically assists in these types of calculations. The 1bp resolution data is masked data, i.e. the 'intensity' values can only be 0 or 1. Then the DFs are multiplied and grouped by their previous starts and ends, i.e. each group is now a specified loci in the original data. Using a groupby operation and a mean calculation we see which group's average is higher than the given 'overlap_pct' value, and if it is we mark that group as overlapping. Parameters other (pd.DataFrame) \u2014 A DF with the relevant data overlap_pct (float, optional) \u2014 The percentage of loci that should overlap between the two datasets Returns (pd.DataFrame, pd.DataFrame) Only the overlapping areas from the self and other DFs method to_1bp_resolution ( multi_chrom=True ) \u2192 DataFrame </> Changes the coordinates of the given DF to have 1bp resolution. function ecodam_py.bedgraph . pad_with_zeros ( nfr , chrom ) </> Adds zero entries for loci which are not included in one of the given DFs function ecodam_py.bedgraph . intervals_to_1bp_mask ( start , end , orig_groups ) \u2192 (ndarray, ndarray) </> Generate a new 1bp BedGraph and keep information of the original distribution and sources of data. function ecodam_py.bedgraph . put_dfs_on_even_grounds ( dfs ) \u2192 iterable of DataFrame </> Asserts overlap of all given DataFrames. An accompanying function to 'put_on_even_grounds' that does the heavy lifting. function ecodam_py.bedgraph . equalize_loci ( first , second ) </> Generate an approximately-equal loci DFs from the two given ones. This function is a mini-pipeline designed to bring two DFs to have the same loci. It works great with a small caveat - the returned 'even' DFs might have 'holes' that aren't covered by any loci. In other words, the loci of the DFs don't necessarily populate all of the loci from the starting point to the ending one. Again, this is only relevant to the 'even' entry, the 1bp resolution copy obviously does cover all BP. Parameters first, second : pd.DataFrame \u2014 Two DFs that have to be equalized Returns (Equalized, Equalized) An even variant (same as the original but only with relevant loci) and 1 bp resolution version with the groups (original loci) marked Raises RuntimeError \u2014 If the DFs cannot be coerced to have the same loci class ecodam_py.bedgraph . BedGraphFile ( file , header=True ) </> A BedGraphFile file which can be manipulated an displayed. The init function will also normalize column names for easier processing down the pipeline. Parameters file (pathlib.Path) \u2014 Data as BedGraph to read header (bool, optional) \u2014 Whether the file contains a header or not Methods add_center_locus ( ) \u2014 Adds a center point to each segment of a molecule. </> convert_df_to_da ( ) \u2014 Create a more complex representation of the tracks as an xr.DataArray. </> smooth ( window ) \u2014 Smooths out the data with the given-sized window. </> method add_center_locus ( ) </> Adds a center point to each segment of a molecule. This is done so that the intensity value can be assigned to a specific base for easier processing. method convert_df_to_da ( ) </> Create a more complex representation of the tracks as an xr.DataArray. The array is a two dimensional array with coordinates that denote the molecule name and its brightness at any given locus. The method creates a new self.dataarray attribute which contains this DataArray. method smooth ( window=1000 ) </> Smooths out the data with the given-sized window. Parameters window (int) \u2014 Smooth data by this window","title":"ecodam_py.bedgraph"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraph","text":"</> This module provides the basic building blocks for working with BedGraphs Attributes Equalized \u2014 A unique return type for equalize_loci Classes Equalized \u2014 Equalized(even, at_1bp, groups) </> BedGraphAccessor \u2014 Introduces a .bg accessor to DataFrame which provides unique capabilities for the DF, including new methods and properties. </> BedGraphFile \u2014 A BedGraphFile file which can be manipulated an displayed. </> Functions equalize_loci ( first , second ) (Equalized, Equalized) \u2014 Generate an approximately-equal loci DFs from the two given ones. </> intervals_to_1bp_mask ( start , end , orig_groups ) (ndarray, ndarray) \u2014 Generate a new 1bp BedGraph and keep information of the original distribution and sources of data. </> pad_with_zeros ( nfr , chrom ) \u2014 Adds zero entries for loci which are not included in one of the given DFs </> put_dfs_on_even_grounds ( dfs ) (iterable of DataFrame) \u2014 Asserts overlap of all given DataFrames. </> class","title":"ecodam_py.bedgraph"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphequalized","text":"</> Bases tuple Equalized(even, at_1bp, groups) class","title":"ecodam_py.bedgraph.Equalized"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphaccessor","text":"</> Introduces a .bg accessor to DataFrame which provides unique capabilities for the DF, including new methods and properties. To use them, simply import this class to your current scope. Methods add_chr ( chr_ ) (pd.DataFrame) \u2014 Adds a 'chr' column to a copied DF. </> columns_to_index ( ) (pd.DataFrame) \u2014 Encodes the loci information in the index of the DF. </> index_to_columns ( ) (A modified DF with two new columns for the locus info) \u2014 Encodes the loci information in two columns of the DF. </> serialize ( fname , mode ) \u2014 Writes the BedGraph to disk. </> to_1bp_resolution ( multi_chrom ) (DataFrame) \u2014 Changes the coordinates of the given DF to have 1bp resolution. </> unweighted_overlap ( other ) (pd.DataFrame, pd.DataFrame) \u2014 Find the overlapping parts of self and other and return these areas. </> weighted_overlap ( other , overlap_pct ) (pd.DataFrame, pd.DataFrame) \u2014 Find the overlapping parts of the two DFs with at least 'overlap_pct' amount of overlap. </> method","title":"ecodam_py.bedgraph.BedGraphAccessor"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphaccessorindex_to_columns","text":"</> Encodes the loci information in two columns of the DF. This method takes a DF that has its locus information encoded in its index, possibly as a result of calling columns_to_index on it, and changes it so that its index becomes a standard RangeIndex and the loci infomration is kept in two np.int64 columns, 'start_locus' and 'end_locus'. method","title":"ecodam_py.bedgraph.BedGraphAccessor.index_to_columns"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphaccessorcolumns_to_index","text":"</> Encodes the loci information in the index of the DF. This method takes a DF that has its locus information as columns, i.e. 'start_locus' and 'end_locus', and moves these columns to a new pd.IntervalIndex instead. Returns (pd.DataFrame) A new DF with an IntervalIndex instead of the 'start_locus' and 'end_locus' columns method","title":"ecodam_py.bedgraph.BedGraphAccessor.columns_to_index"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphaccessoradd_chr","text":"</> Adds a 'chr' column to a copied DF. Useful in certain internal situations mostly. Parameters chr (str, optional) \u2014 The chromosome name to add Returns (pd.DataFrame) A new DF with the new column method","title":"ecodam_py.bedgraph.BedGraphAccessor.add_chr"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphaccessorserialize","text":"</> Writes the BedGraph to disk. This method first normalizes the BedGraph and then writes it to disk with the given write mode. Parameters fname (pathlib.Path) \u2014 Filename to write to mode (str) \u2014 File writing mode (similar to the built-in open()). By default 'w' method","title":"ecodam_py.bedgraph.BedGraphAccessor.serialize"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphaccessorunweighted_overlap","text":"</> Find the overlapping parts of self and other and return these areas. This method looks for overlapping parts of the two BedGraph DFs and returns only the relevant slices of these two objects. The overlap is called 'unweighted' because it considers an area to be overlapping even if a single BP overlaps between the two loci. This might seem odd, but it's helpful since we usually work with BedGraph files that have the same coordinates, so this method is good enough. Use the (slower) 'weighted_overlap' method if you need to assert that the overlap is signifanct in terms of BP counts. Parameters other (pd.DataFrame) \u2014 A BedGraph DF Returns (pd.DataFrame, pd.DataFrame) The 'self' and 'other' DFs only at the rows that overlap method","title":"ecodam_py.bedgraph.BedGraphAccessor.unweighted_overlap"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphaccessorweighted_overlap","text":"</> Find the overlapping parts of the two DFs with at least 'overlap_pct' amount of overlap. In the first phase of this method, the two DFs are put on the same coordinates so that they could be compared in a viable manner. Currently the way its done is to move them to 1bp resolution which automatically assists in these types of calculations. The 1bp resolution data is masked data, i.e. the 'intensity' values can only be 0 or 1. Then the DFs are multiplied and grouped by their previous starts and ends, i.e. each group is now a specified loci in the original data. Using a groupby operation and a mean calculation we see which group's average is higher than the given 'overlap_pct' value, and if it is we mark that group as overlapping. Parameters other (pd.DataFrame) \u2014 A DF with the relevant data overlap_pct (float, optional) \u2014 The percentage of loci that should overlap between the two datasets Returns (pd.DataFrame, pd.DataFrame) Only the overlapping areas from the self and other DFs method","title":"ecodam_py.bedgraph.BedGraphAccessor.weighted_overlap"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphaccessorto_1bp_resolution","text":"</> Changes the coordinates of the given DF to have 1bp resolution. function","title":"ecodam_py.bedgraph.BedGraphAccessor.to_1bp_resolution"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphpad_with_zeros","text":"</> Adds zero entries for loci which are not included in one of the given DFs function","title":"ecodam_py.bedgraph.pad_with_zeros"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphintervals_to_1bp_mask","text":"</> Generate a new 1bp BedGraph and keep information of the original distribution and sources of data. function","title":"ecodam_py.bedgraph.intervals_to_1bp_mask"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphput_dfs_on_even_grounds","text":"</> Asserts overlap of all given DataFrames. An accompanying function to 'put_on_even_grounds' that does the heavy lifting. function","title":"ecodam_py.bedgraph.put_dfs_on_even_grounds"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphequalize_loci","text":"</> Generate an approximately-equal loci DFs from the two given ones. This function is a mini-pipeline designed to bring two DFs to have the same loci. It works great with a small caveat - the returned 'even' DFs might have 'holes' that aren't covered by any loci. In other words, the loci of the DFs don't necessarily populate all of the loci from the starting point to the ending one. Again, this is only relevant to the 'even' entry, the 1bp resolution copy obviously does cover all BP. Parameters first, second : pd.DataFrame \u2014 Two DFs that have to be equalized Returns (Equalized, Equalized) An even variant (same as the original but only with relevant loci) and 1 bp resolution version with the groups (original loci) marked Raises RuntimeError \u2014 If the DFs cannot be coerced to have the same loci class","title":"ecodam_py.bedgraph.equalize_loci"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphfile","text":"</> A BedGraphFile file which can be manipulated an displayed. The init function will also normalize column names for easier processing down the pipeline. Parameters file (pathlib.Path) \u2014 Data as BedGraph to read header (bool, optional) \u2014 Whether the file contains a header or not Methods add_center_locus ( ) \u2014 Adds a center point to each segment of a molecule. </> convert_df_to_da ( ) \u2014 Create a more complex representation of the tracks as an xr.DataArray. </> smooth ( window ) \u2014 Smooths out the data with the given-sized window. </> method","title":"ecodam_py.bedgraph.BedGraphFile"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphfileadd_center_locus","text":"</> Adds a center point to each segment of a molecule. This is done so that the intensity value can be assigned to a specific base for easier processing. method","title":"ecodam_py.bedgraph.BedGraphFile.add_center_locus"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphfileconvert_df_to_da","text":"</> Create a more complex representation of the tracks as an xr.DataArray. The array is a two dimensional array with coordinates that denote the molecule name and its brightness at any given locus. The method creates a new self.dataarray attribute which contains this DataArray. method","title":"ecodam_py.bedgraph.BedGraphFile.convert_df_to_da"},{"location":"api/ecodam_py.bedgraph/#ecodam_pybedgraphbedgraphfilesmooth","text":"</> Smooths out the data with the given-sized window. Parameters window (int) \u2014 Smooth data by this window","title":"ecodam_py.bedgraph.BedGraphFile.smooth"},{"location":"api/ecodam_py.compare_ecodam_atac/","text":"module ecodam_py . compare_ecodam_atac </> Functions put_on_even_grounds ( beds ) (List[BedGraphFile]) \u2014 Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. </> function ecodam_py.compare_ecodam_atac . put_on_even_grounds ( beds ) </> Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. The function will trim the datasets but will try to leave as many base pairs as possible. Parameters beds (List[BedGraphFile]) \u2014 Data to trim in the order Eco, ATAC, Naked","title":"ecodam_py.compare_ecodam_atac"},{"location":"api/ecodam_py.compare_ecodam_atac/#ecodam_pycompare_ecodam_atac","text":"</> Functions put_on_even_grounds ( beds ) (List[BedGraphFile]) \u2014 Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. </> function","title":"ecodam_py.compare_ecodam_atac"},{"location":"api/ecodam_py.compare_ecodam_atac/#ecodam_pycompare_ecodam_atacput_on_even_grounds","text":"</> Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. The function will trim the datasets but will try to leave as many base pairs as possible. Parameters beds (List[BedGraphFile]) \u2014 Data to trim in the order Eco, ATAC, Naked","title":"ecodam_py.compare_ecodam_atac.put_on_even_grounds"},{"location":"api/ecodam_py.eco_atac_normalization/","text":"module ecodam_py . eco_atac_normalization </> Functions accompanying the 'normalize_eco_atac.ipynb' notebook. This is a library-like assortment of functions with no clear start or end, so don't expect any logic that can make it seem ordered. Moreover, some of the functionality exists in more than one place due to the chaotic way in which this project was created. Functions concat_clusters ( eco , atac , clusters ) (DataFrame, DataFrame) \u2014 Extracts the data from the 'eco' and 'atac' DFs using the indices in 'clusters'. </> convert_to_intervalindex ( beds ) (List[BedGraphFile]) \u2014 Creates an IntervalIndex index for each BedGraphFile in the given list. </> equalize_distribs ( dfs , atac ) \u2014 Change the data limits of the DFs to match the ATAC one. </> expand_seeds ( data , cluster_thresh ) (DataFrame) \u2014 Generate a bedgraph-like DF from the given list of seeds. </> find_closest_diff ( eco , atac , thresh ) (Tuple[pd.Series, pd.Series]) \u2014 Subtracts the two given tracks and returns them only at the locations closer than thresh. </> generate_df_for_theo_correlation_comparison ( data , theo , nquants ) (pd.DataFrame) \u2014 Creates a long form dataframe that can be used to show correlation between the given theoretical data and some other arbitraty DF. </> generate_intervals_1kb ( data ) (pd.IntervalIndex) \u2014 Creates evenly spaced intervals from existing DFs. </> get_peak_indices ( peaks , data ) (ndarray) \u2014 Iterate over the peaks and extract the data index at these points. </> iter_over_bedgraphs_chromosomes ( *args ) (An iterator that yields a tuple of the DFs only at a certain chromosome) \u2014 Constructs an chromosome iterator for each DF in the given args. </> match_histograms ( eco , atac ) \u2014 Wrapper for the match_histograms scikit-image function </> normalize_group_peaks_single_factor ( peaks , data , norm_to ) \u2014 Multiplies the given data by some norm factor, or finds that norm factor. </> normalize_with_site_density ( chrom , naked , theo ) (pd.DataFrame, pd.DataFrame, pd.DataFrame) \u2014 Use the site density as supplied in the theoretical DF to normalize the values in the other two DFs. </> plot_bg ( eco , naked , atac ) \u2014 Plots a BedGraphFile's DF </> prepare_site_density_for_norm ( theo , quantile ) (blacklist_mask : pd.Series) \u2014 Generate a correct theoretical data mask for normalization. </> preprocess_bedgraph ( paths ) (List[BedGraphFile]) \u2014 Run basic pre-processing for the filenames. </> put_on_even_grounds ( beds ) (List[BedGraphFile]) \u2014 Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. </> show_ridge_plot ( df , name ) (sns.FacetGrid) \u2014 Shows the distribution of the data for different categories. </> subtract_background_with_theo ( data , no_sites ) (DataFrame) \u2014 Remove background component and zero-information bins from the data. </> write_intindex_to_disk ( data , fname , chr_ ) \u2014 Writes the data to disk as a bedgraph. </> function ecodam_py.eco_atac_normalization . put_on_even_grounds ( beds ) </> Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. The function will trim the datasets but will try to leave as many base pairs as possible. Parameters beds (List[BedGraphFile]) \u2014 Data to trim in the order Eco, ATAC, Naked function ecodam_py.eco_atac_normalization . convert_to_intervalindex ( beds ) </> Creates an IntervalIndex index for each BedGraphFile in the given list. An IntervalIndex object is very suitable for the task of describing the way tracks are distributed, and this function's goal is to bulk translate the existing information about intensities into these intervals. As a rule of thumb I decided to make the interval closed on the left end. function ecodam_py.eco_atac_normalization . generate_intervals_1kb ( data ) </> Creates evenly spaced intervals from existing DFs. The given DF should contain an IntervalIndex and this function is a step in making these intervals evenly spaced and equal to other intervals. Parameters data (pd.DataFrame) \u2014 A DF of a BedGraphFile file after its index was converted to an IntervalIndex Returns (pd.IntervalIndex) Evenly spaced IntervalIndex at 1000 bp. function ecodam_py.eco_atac_normalization . equalize_distribs ( dfs , atac ) </> Change the data limits of the DFs to match the ATAC one. This normalization step helps us compare datasets that are displayed with the same values. function ecodam_py.eco_atac_normalization . match_histograms ( eco , atac ) </> Wrapper for the match_histograms scikit-image function function ecodam_py.eco_atac_normalization . plot_bg ( eco , naked , atac ) </> Plots a BedGraphFile's DF function ecodam_py.eco_atac_normalization . find_closest_diff ( eco , atac , thresh=0.5 ) </> Subtracts the two given tracks and returns them only at the locations closer than thresh. For the given Series objects the function runs an elementwise subtraction and finds the closest areas of the two tracks. Thresh is used as an absolute value and not a relative one. Parameters thresh (float) \u2014 The value below which the datasets are considered close eco, atac : pd.Series \u2014 Two tracks to diff Returns (Tuple[pd.Series, pd.Series]) The two original datasets but only at the locations at which hey match function ecodam_py.eco_atac_normalization . write_intindex_to_disk ( data , fname , chr_='chr15' ) </> Writes the data to disk as a bedgraph. Serializes the data assuming that it has an IntervalIndex as its locus data. The serialization format is basically a bedgraph, i.e. four columns - chr, start, end, intensity. Parameters data (Union[pd.Series, pd.DataFrame]) \u2014 Bedgraph-like data to serialize fname (pathlib.Path) \u2014 New filename chr_ (str) \u2014 Chromosome function ecodam_py.eco_atac_normalization . expand_seeds ( data , cluster_thresh=15000 ) \u2192 DataFrame </> Generate a bedgraph-like DF from the given list of seeds. Each seed is marked in the incoming data by start and end coordinates in its index. The function detect seed clusters, i.e. areas where at least two seeds exist less than 'cluster_thresh' BP apart, and then iterates over all of these clusters and concatenate them together to a new BedGraphFile like structure. function ecodam_py.eco_atac_normalization . concat_clusters ( eco , atac , clusters ) \u2192 (DataFrame, DataFrame) </> Extracts the data from the 'eco' and 'atac' DFs using the indices in 'clusters'. This function iterates over each cluster as defined in the 'clusters' input and adds the corresponding values from the Chromatin and ATAC datasets into new DataFrames that only contain these clusters. function ecodam_py.eco_atac_normalization . prepare_site_density_for_norm ( theo , quantile=0.1 ) </> Generate a correct theoretical data mask for normalization. The theoretical data is first filtered so that the low quality data is taken out. Then the rest of the data is normalized so that its mean is one. This is sent, as a mask and as the normalization factor, back to the caller so that it could be arbitrarily applied on other data. Parameters theo (pd.DataFrame) \u2014 Site density values quantile (float, optional) \u2014 The quantile that thresholds the data quality. Data below this quantile will be filtered out Returns (blacklist_mask : pd.Series) Boolean mask for the relevant parts in the chromosome _by : pd.DataFrame Values to multiply with the data for it to be normalized function ecodam_py.eco_atac_normalization . normalize_with_site_density ( chrom , naked , theo ) </> Use the site density as supplied in the theoretical DF to normalize the values in the other two DFs. First we generate the mask that will be used to discard unneeded events. The mean of the rest of the data is then normalized to one, since the units are arbitrary and it's easier to work with these values (the theo values are relatively low). Then we can apply the same mask to the rest of the data and normalize it by their corresponding theoretical value. Parameters theo (pd.DataFrame) \u2014 The data we normalize with chrom, naked : pd.DataFrame \u2014 The data to be normalized Returns (pd.DataFrame, pd.DataFrame, pd.DataFrame) The normalized DFs only at the relevant locations after normalization function ecodam_py.eco_atac_normalization . preprocess_bedgraph ( paths ) </> Run basic pre-processing for the filenames. The function generates a BedGraphFile object and sorts its data attribute. Parameters paths (List[pathlib.Path]) \u2014 Paths of BedGraphFile files to read function ecodam_py.eco_atac_normalization . subtract_background_with_theo ( data , no_sites ) \u2192 DataFrame </> Remove background component and zero-information bins from the data. The bins containing a theoretical value of zero are basically garbage, so we can safely discard them after using them to calculate the noise levels. function ecodam_py.eco_atac_normalization . generate_df_for_theo_correlation_comparison ( data , theo , nquants=5 ) </> Creates a long form dataframe that can be used to show correlation between the given theoretical data and some other arbitraty DF. The goal here is to see how do the different values of the data variable, be it the naked data or the chromatin one, correlate with the different levels of the theoretical data. Parameters data (pd.DataFrame) \u2014 The DF from a BedGraphFile object theo (pd.DataFrame) \u2014 Data to compare to nquants (int) \u2014 Number of quantiles to display Returns (pd.DataFrame) Long-form DF that can be displayed as a Ridge plot function ecodam_py.eco_atac_normalization . show_ridge_plot ( df , name='naked' ) </> Shows the distribution of the data for different categories. Using the output from generate_df_for_theo_correlation_comparison we group the data per quantile and display it in several different distribution plots. Taken almost directly from the seaborn gallery. Parameters df (pd.DataFrame) \u2014 Usually from generate_df_for_theo_correlation_comparison name (str, optional) \u2014 Name of the dataset, i.e. 'naked' or 'chromatin' function ecodam_py.eco_atac_normalization . get_peak_indices ( peaks , data ) \u2192 ndarray </> Iterate over the peaks and extract the data index at these points. function ecodam_py.eco_atac_normalization . normalize_group_peaks_single_factor ( peaks , data , norm_to=None ) </> Multiplies the given data by some norm factor, or finds that norm factor. We wish to normalize the two groups, naked and chrom, using the peak data. This function finds the median value of the peaks and uses that value as the go-to target for the other target. generator ecodam_py.eco_atac_normalization . iter_over_bedgraphs_chromosomes ( *args ) </> Constructs an chromosome iterator for each DF in the given args. The function can take one or more DFs with the chromosome column and bind them together for a generator that only yields the relevant rows of each DF every time.","title":"ecodam_py.eco_atac_normalization"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalization","text":"</> Functions accompanying the 'normalize_eco_atac.ipynb' notebook. This is a library-like assortment of functions with no clear start or end, so don't expect any logic that can make it seem ordered. Moreover, some of the functionality exists in more than one place due to the chaotic way in which this project was created. Functions concat_clusters ( eco , atac , clusters ) (DataFrame, DataFrame) \u2014 Extracts the data from the 'eco' and 'atac' DFs using the indices in 'clusters'. </> convert_to_intervalindex ( beds ) (List[BedGraphFile]) \u2014 Creates an IntervalIndex index for each BedGraphFile in the given list. </> equalize_distribs ( dfs , atac ) \u2014 Change the data limits of the DFs to match the ATAC one. </> expand_seeds ( data , cluster_thresh ) (DataFrame) \u2014 Generate a bedgraph-like DF from the given list of seeds. </> find_closest_diff ( eco , atac , thresh ) (Tuple[pd.Series, pd.Series]) \u2014 Subtracts the two given tracks and returns them only at the locations closer than thresh. </> generate_df_for_theo_correlation_comparison ( data , theo , nquants ) (pd.DataFrame) \u2014 Creates a long form dataframe that can be used to show correlation between the given theoretical data and some other arbitraty DF. </> generate_intervals_1kb ( data ) (pd.IntervalIndex) \u2014 Creates evenly spaced intervals from existing DFs. </> get_peak_indices ( peaks , data ) (ndarray) \u2014 Iterate over the peaks and extract the data index at these points. </> iter_over_bedgraphs_chromosomes ( *args ) (An iterator that yields a tuple of the DFs only at a certain chromosome) \u2014 Constructs an chromosome iterator for each DF in the given args. </> match_histograms ( eco , atac ) \u2014 Wrapper for the match_histograms scikit-image function </> normalize_group_peaks_single_factor ( peaks , data , norm_to ) \u2014 Multiplies the given data by some norm factor, or finds that norm factor. </> normalize_with_site_density ( chrom , naked , theo ) (pd.DataFrame, pd.DataFrame, pd.DataFrame) \u2014 Use the site density as supplied in the theoretical DF to normalize the values in the other two DFs. </> plot_bg ( eco , naked , atac ) \u2014 Plots a BedGraphFile's DF </> prepare_site_density_for_norm ( theo , quantile ) (blacklist_mask : pd.Series) \u2014 Generate a correct theoretical data mask for normalization. </> preprocess_bedgraph ( paths ) (List[BedGraphFile]) \u2014 Run basic pre-processing for the filenames. </> put_on_even_grounds ( beds ) (List[BedGraphFile]) \u2014 Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. </> show_ridge_plot ( df , name ) (sns.FacetGrid) \u2014 Shows the distribution of the data for different categories. </> subtract_background_with_theo ( data , no_sites ) (DataFrame) \u2014 Remove background component and zero-information bins from the data. </> write_intindex_to_disk ( data , fname , chr_ ) \u2014 Writes the data to disk as a bedgraph. </> function","title":"ecodam_py.eco_atac_normalization"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationput_on_even_grounds","text":"</> Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. The function will trim the datasets but will try to leave as many base pairs as possible. Parameters beds (List[BedGraphFile]) \u2014 Data to trim in the order Eco, ATAC, Naked function","title":"ecodam_py.eco_atac_normalization.put_on_even_grounds"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationconvert_to_intervalindex","text":"</> Creates an IntervalIndex index for each BedGraphFile in the given list. An IntervalIndex object is very suitable for the task of describing the way tracks are distributed, and this function's goal is to bulk translate the existing information about intensities into these intervals. As a rule of thumb I decided to make the interval closed on the left end. function","title":"ecodam_py.eco_atac_normalization.convert_to_intervalindex"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationgenerate_intervals_1kb","text":"</> Creates evenly spaced intervals from existing DFs. The given DF should contain an IntervalIndex and this function is a step in making these intervals evenly spaced and equal to other intervals. Parameters data (pd.DataFrame) \u2014 A DF of a BedGraphFile file after its index was converted to an IntervalIndex Returns (pd.IntervalIndex) Evenly spaced IntervalIndex at 1000 bp. function","title":"ecodam_py.eco_atac_normalization.generate_intervals_1kb"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationequalize_distribs","text":"</> Change the data limits of the DFs to match the ATAC one. This normalization step helps us compare datasets that are displayed with the same values. function","title":"ecodam_py.eco_atac_normalization.equalize_distribs"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationmatch_histograms","text":"</> Wrapper for the match_histograms scikit-image function function","title":"ecodam_py.eco_atac_normalization.match_histograms"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationplot_bg","text":"</> Plots a BedGraphFile's DF function","title":"ecodam_py.eco_atac_normalization.plot_bg"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationfind_closest_diff","text":"</> Subtracts the two given tracks and returns them only at the locations closer than thresh. For the given Series objects the function runs an elementwise subtraction and finds the closest areas of the two tracks. Thresh is used as an absolute value and not a relative one. Parameters thresh (float) \u2014 The value below which the datasets are considered close eco, atac : pd.Series \u2014 Two tracks to diff Returns (Tuple[pd.Series, pd.Series]) The two original datasets but only at the locations at which hey match function","title":"ecodam_py.eco_atac_normalization.find_closest_diff"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationwrite_intindex_to_disk","text":"</> Writes the data to disk as a bedgraph. Serializes the data assuming that it has an IntervalIndex as its locus data. The serialization format is basically a bedgraph, i.e. four columns - chr, start, end, intensity. Parameters data (Union[pd.Series, pd.DataFrame]) \u2014 Bedgraph-like data to serialize fname (pathlib.Path) \u2014 New filename chr_ (str) \u2014 Chromosome function","title":"ecodam_py.eco_atac_normalization.write_intindex_to_disk"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationexpand_seeds","text":"</> Generate a bedgraph-like DF from the given list of seeds. Each seed is marked in the incoming data by start and end coordinates in its index. The function detect seed clusters, i.e. areas where at least two seeds exist less than 'cluster_thresh' BP apart, and then iterates over all of these clusters and concatenate them together to a new BedGraphFile like structure. function","title":"ecodam_py.eco_atac_normalization.expand_seeds"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationconcat_clusters","text":"</> Extracts the data from the 'eco' and 'atac' DFs using the indices in 'clusters'. This function iterates over each cluster as defined in the 'clusters' input and adds the corresponding values from the Chromatin and ATAC datasets into new DataFrames that only contain these clusters. function","title":"ecodam_py.eco_atac_normalization.concat_clusters"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationprepare_site_density_for_norm","text":"</> Generate a correct theoretical data mask for normalization. The theoretical data is first filtered so that the low quality data is taken out. Then the rest of the data is normalized so that its mean is one. This is sent, as a mask and as the normalization factor, back to the caller so that it could be arbitrarily applied on other data. Parameters theo (pd.DataFrame) \u2014 Site density values quantile (float, optional) \u2014 The quantile that thresholds the data quality. Data below this quantile will be filtered out Returns (blacklist_mask : pd.Series) Boolean mask for the relevant parts in the chromosome _by : pd.DataFrame Values to multiply with the data for it to be normalized function","title":"ecodam_py.eco_atac_normalization.prepare_site_density_for_norm"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationnormalize_with_site_density","text":"</> Use the site density as supplied in the theoretical DF to normalize the values in the other two DFs. First we generate the mask that will be used to discard unneeded events. The mean of the rest of the data is then normalized to one, since the units are arbitrary and it's easier to work with these values (the theo values are relatively low). Then we can apply the same mask to the rest of the data and normalize it by their corresponding theoretical value. Parameters theo (pd.DataFrame) \u2014 The data we normalize with chrom, naked : pd.DataFrame \u2014 The data to be normalized Returns (pd.DataFrame, pd.DataFrame, pd.DataFrame) The normalized DFs only at the relevant locations after normalization function","title":"ecodam_py.eco_atac_normalization.normalize_with_site_density"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationpreprocess_bedgraph","text":"</> Run basic pre-processing for the filenames. The function generates a BedGraphFile object and sorts its data attribute. Parameters paths (List[pathlib.Path]) \u2014 Paths of BedGraphFile files to read function","title":"ecodam_py.eco_atac_normalization.preprocess_bedgraph"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationsubtract_background_with_theo","text":"</> Remove background component and zero-information bins from the data. The bins containing a theoretical value of zero are basically garbage, so we can safely discard them after using them to calculate the noise levels. function","title":"ecodam_py.eco_atac_normalization.subtract_background_with_theo"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationgenerate_df_for_theo_correlation_comparison","text":"</> Creates a long form dataframe that can be used to show correlation between the given theoretical data and some other arbitraty DF. The goal here is to see how do the different values of the data variable, be it the naked data or the chromatin one, correlate with the different levels of the theoretical data. Parameters data (pd.DataFrame) \u2014 The DF from a BedGraphFile object theo (pd.DataFrame) \u2014 Data to compare to nquants (int) \u2014 Number of quantiles to display Returns (pd.DataFrame) Long-form DF that can be displayed as a Ridge plot function","title":"ecodam_py.eco_atac_normalization.generate_df_for_theo_correlation_comparison"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationshow_ridge_plot","text":"</> Shows the distribution of the data for different categories. Using the output from generate_df_for_theo_correlation_comparison we group the data per quantile and display it in several different distribution plots. Taken almost directly from the seaborn gallery. Parameters df (pd.DataFrame) \u2014 Usually from generate_df_for_theo_correlation_comparison name (str, optional) \u2014 Name of the dataset, i.e. 'naked' or 'chromatin' function","title":"ecodam_py.eco_atac_normalization.show_ridge_plot"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationget_peak_indices","text":"</> Iterate over the peaks and extract the data index at these points. function","title":"ecodam_py.eco_atac_normalization.get_peak_indices"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationnormalize_group_peaks_single_factor","text":"</> Multiplies the given data by some norm factor, or finds that norm factor. We wish to normalize the two groups, naked and chrom, using the peak data. This function finds the median value of the peaks and uses that value as the go-to target for the other target. generator","title":"ecodam_py.eco_atac_normalization.normalize_group_peaks_single_factor"},{"location":"api/ecodam_py.eco_atac_normalization/#ecodam_pyeco_atac_normalizationiter_over_bedgraphs_chromosomes","text":"</> Constructs an chromosome iterator for each DF in the given args. The function can take one or more DFs with the chromosome column and bind them together for a generator that only yields the relevant rows of each DF every time.","title":"ecodam_py.eco_atac_normalization.iter_over_bedgraphs_chromosomes"},{"location":"api/ecodam_py/","text":"package ecodam_py </> Compare and visualize EcoDAM performance https://github.com/ebensteinLab/EcoDAM/ module ecodam_py . bedgraph </> This module provides the basic building blocks for working with BedGraphs Attributes Equalized \u2014 A unique return type for equalize_loci Classes Equalized \u2014 Equalized(even, at_1bp, groups) </> BedGraphAccessor \u2014 Introduces a .bg accessor to DataFrame which provides unique capabilities for the DF, including new methods and properties. </> BedGraphFile \u2014 A BedGraphFile file which can be manipulated an displayed. </> Functions equalize_loci ( first , second ) (Equalized, Equalized) \u2014 Generate an approximately-equal loci DFs from the two given ones. </> intervals_to_1bp_mask ( start , end , orig_groups ) (ndarray, ndarray) \u2014 Generate a new 1bp BedGraph and keep information of the original distribution and sources of data. </> pad_with_zeros ( nfr , chrom ) \u2014 Adds zero entries for loci which are not included in one of the given DFs </> put_dfs_on_even_grounds ( dfs ) (iterable of DataFrame) \u2014 Asserts overlap of all given DataFrames. </> module ecodam_py . parse_gene_coding </> A filteration script written pretty specifically for some GTF file that Gil needed to be parsed and filtered. It takes about an hour or two to run on the Irys server due to the slow iteration step that is performed internally by the gffutils library. It also reverses the start and end loci on the rows with the - strand, as seen in the find_minus_primer function. module ecodam_py . peak_calling </> Functions accompanying the 'peak_calling' notebook. Functions color_peak_surroundings ( atac_split , atac_indices , eco_split , eco_indices ) \u2014 Generate a plot for the peak-centered BedGraphFile data. </> define_peak_surroundings ( atac , atac_peak_indices , eco , eco_peak_indices ) \u2014 Aggregator function to prettify notebook's code. </> preprocess_data ( fname ) (BedGraphFile) \u2014 A series of simple steps that should be done automatically to BedGraphFile data. </> module ecodam_py . eco_atac_normalization </> Functions accompanying the 'normalize_eco_atac.ipynb' notebook. This is a library-like assortment of functions with no clear start or end, so don't expect any logic that can make it seem ordered. Moreover, some of the functionality exists in more than one place due to the chaotic way in which this project was created. Functions concat_clusters ( eco , atac , clusters ) (DataFrame, DataFrame) \u2014 Extracts the data from the 'eco' and 'atac' DFs using the indices in 'clusters'. </> convert_to_intervalindex ( beds ) (List[BedGraphFile]) \u2014 Creates an IntervalIndex index for each BedGraphFile in the given list. </> equalize_distribs ( dfs , atac ) \u2014 Change the data limits of the DFs to match the ATAC one. </> expand_seeds ( data , cluster_thresh ) (DataFrame) \u2014 Generate a bedgraph-like DF from the given list of seeds. </> find_closest_diff ( eco , atac , thresh ) (Tuple[pd.Series, pd.Series]) \u2014 Subtracts the two given tracks and returns them only at the locations closer than thresh. </> generate_df_for_theo_correlation_comparison ( data , theo , nquants ) (pd.DataFrame) \u2014 Creates a long form dataframe that can be used to show correlation between the given theoretical data and some other arbitraty DF. </> generate_intervals_1kb ( data ) (pd.IntervalIndex) \u2014 Creates evenly spaced intervals from existing DFs. </> get_peak_indices ( peaks , data ) (ndarray) \u2014 Iterate over the peaks and extract the data index at these points. </> iter_over_bedgraphs_chromosomes ( *args ) (An iterator that yields a tuple of the DFs only at a certain chromosome) \u2014 Constructs an chromosome iterator for each DF in the given args. </> match_histograms ( eco , atac ) \u2014 Wrapper for the match_histograms scikit-image function </> normalize_group_peaks_single_factor ( peaks , data , norm_to ) \u2014 Multiplies the given data by some norm factor, or finds that norm factor. </> normalize_with_site_density ( chrom , naked , theo ) (pd.DataFrame, pd.DataFrame, pd.DataFrame) \u2014 Use the site density as supplied in the theoretical DF to normalize the values in the other two DFs. </> plot_bg ( eco , naked , atac ) \u2014 Plots a BedGraphFile's DF </> prepare_site_density_for_norm ( theo , quantile ) (blacklist_mask : pd.Series) \u2014 Generate a correct theoretical data mask for normalization. </> preprocess_bedgraph ( paths ) (List[BedGraphFile]) \u2014 Run basic pre-processing for the filenames. </> put_on_even_grounds ( beds ) (List[BedGraphFile]) \u2014 Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. </> show_ridge_plot ( df , name ) (sns.FacetGrid) \u2014 Shows the distribution of the data for different categories. </> subtract_background_with_theo ( data , no_sites ) (DataFrame) \u2014 Remove background component and zero-information bins from the data. </> write_intindex_to_disk ( data , fname , chr_ ) \u2014 Writes the data to disk as a bedgraph. </> module ecodam_py . pybam </> Awesome people who have directly contributed to the project: Jon Palmer - Bug finder & advice on project direction Mahmut Uludag - Bug finder Help: print pybam.wat Github: http://github.com/JohnLonginotto/pybam This code was written by John Longinotto, a PhD student of the Pospisilik Lab at the Max Planck Institute of Immunbiology & Epigenetics, Freiburg. My PhD is funded by the Deutsches Epigenom Programm (DEEP), and the Max Planck IMPRS Program. I study Adipose Biology and Circadian Rhythm in mice, although it seems these days I spend most of my time at the computer :-) Classes read \u2014 [ Dynamic Parser Example ] for alignment in pybam.read('/my/data.bam'): print alignment.sam_seq </> PybamWarn \u2014 Common base class for all non-exit exceptions. </> PybamError \u2014 Common base class for all non-exit exceptions. </>","title":"ecodam_py"},{"location":"api/ecodam_py/#ecodam_py","text":"</> Compare and visualize EcoDAM performance https://github.com/ebensteinLab/EcoDAM/ module","title":"ecodam_py"},{"location":"api/ecodam_py/#ecodam_pybedgraph","text":"</> This module provides the basic building blocks for working with BedGraphs Attributes Equalized \u2014 A unique return type for equalize_loci Classes Equalized \u2014 Equalized(even, at_1bp, groups) </> BedGraphAccessor \u2014 Introduces a .bg accessor to DataFrame which provides unique capabilities for the DF, including new methods and properties. </> BedGraphFile \u2014 A BedGraphFile file which can be manipulated an displayed. </> Functions equalize_loci ( first , second ) (Equalized, Equalized) \u2014 Generate an approximately-equal loci DFs from the two given ones. </> intervals_to_1bp_mask ( start , end , orig_groups ) (ndarray, ndarray) \u2014 Generate a new 1bp BedGraph and keep information of the original distribution and sources of data. </> pad_with_zeros ( nfr , chrom ) \u2014 Adds zero entries for loci which are not included in one of the given DFs </> put_dfs_on_even_grounds ( dfs ) (iterable of DataFrame) \u2014 Asserts overlap of all given DataFrames. </> module","title":"ecodam_py.bedgraph"},{"location":"api/ecodam_py/#ecodam_pyparse_gene_coding","text":"</> A filteration script written pretty specifically for some GTF file that Gil needed to be parsed and filtered. It takes about an hour or two to run on the Irys server due to the slow iteration step that is performed internally by the gffutils library. It also reverses the start and end loci on the rows with the - strand, as seen in the find_minus_primer function. module","title":"ecodam_py.parse_gene_coding"},{"location":"api/ecodam_py/#ecodam_pypeak_calling","text":"</> Functions accompanying the 'peak_calling' notebook. Functions color_peak_surroundings ( atac_split , atac_indices , eco_split , eco_indices ) \u2014 Generate a plot for the peak-centered BedGraphFile data. </> define_peak_surroundings ( atac , atac_peak_indices , eco , eco_peak_indices ) \u2014 Aggregator function to prettify notebook's code. </> preprocess_data ( fname ) (BedGraphFile) \u2014 A series of simple steps that should be done automatically to BedGraphFile data. </> module","title":"ecodam_py.peak_calling"},{"location":"api/ecodam_py/#ecodam_pyeco_atac_normalization","text":"</> Functions accompanying the 'normalize_eco_atac.ipynb' notebook. This is a library-like assortment of functions with no clear start or end, so don't expect any logic that can make it seem ordered. Moreover, some of the functionality exists in more than one place due to the chaotic way in which this project was created. Functions concat_clusters ( eco , atac , clusters ) (DataFrame, DataFrame) \u2014 Extracts the data from the 'eco' and 'atac' DFs using the indices in 'clusters'. </> convert_to_intervalindex ( beds ) (List[BedGraphFile]) \u2014 Creates an IntervalIndex index for each BedGraphFile in the given list. </> equalize_distribs ( dfs , atac ) \u2014 Change the data limits of the DFs to match the ATAC one. </> expand_seeds ( data , cluster_thresh ) (DataFrame) \u2014 Generate a bedgraph-like DF from the given list of seeds. </> find_closest_diff ( eco , atac , thresh ) (Tuple[pd.Series, pd.Series]) \u2014 Subtracts the two given tracks and returns them only at the locations closer than thresh. </> generate_df_for_theo_correlation_comparison ( data , theo , nquants ) (pd.DataFrame) \u2014 Creates a long form dataframe that can be used to show correlation between the given theoretical data and some other arbitraty DF. </> generate_intervals_1kb ( data ) (pd.IntervalIndex) \u2014 Creates evenly spaced intervals from existing DFs. </> get_peak_indices ( peaks , data ) (ndarray) \u2014 Iterate over the peaks and extract the data index at these points. </> iter_over_bedgraphs_chromosomes ( *args ) (An iterator that yields a tuple of the DFs only at a certain chromosome) \u2014 Constructs an chromosome iterator for each DF in the given args. </> match_histograms ( eco , atac ) \u2014 Wrapper for the match_histograms scikit-image function </> normalize_group_peaks_single_factor ( peaks , data , norm_to ) \u2014 Multiplies the given data by some norm factor, or finds that norm factor. </> normalize_with_site_density ( chrom , naked , theo ) (pd.DataFrame, pd.DataFrame, pd.DataFrame) \u2014 Use the site density as supplied in the theoretical DF to normalize the values in the other two DFs. </> plot_bg ( eco , naked , atac ) \u2014 Plots a BedGraphFile's DF </> prepare_site_density_for_norm ( theo , quantile ) (blacklist_mask : pd.Series) \u2014 Generate a correct theoretical data mask for normalization. </> preprocess_bedgraph ( paths ) (List[BedGraphFile]) \u2014 Run basic pre-processing for the filenames. </> put_on_even_grounds ( beds ) (List[BedGraphFile]) \u2014 Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. </> show_ridge_plot ( df , name ) (sns.FacetGrid) \u2014 Shows the distribution of the data for different categories. </> subtract_background_with_theo ( data , no_sites ) (DataFrame) \u2014 Remove background component and zero-information bins from the data. </> write_intindex_to_disk ( data , fname , chr_ ) \u2014 Writes the data to disk as a bedgraph. </> module","title":"ecodam_py.eco_atac_normalization"},{"location":"api/ecodam_py/#ecodam_pypybam","text":"</> Awesome people who have directly contributed to the project: Jon Palmer - Bug finder & advice on project direction Mahmut Uludag - Bug finder Help: print pybam.wat Github: http://github.com/JohnLonginotto/pybam This code was written by John Longinotto, a PhD student of the Pospisilik Lab at the Max Planck Institute of Immunbiology & Epigenetics, Freiburg. My PhD is funded by the Deutsches Epigenom Programm (DEEP), and the Max Planck IMPRS Program. I study Adipose Biology and Circadian Rhythm in mice, although it seems these days I spend most of my time at the computer :-) Classes read \u2014 [ Dynamic Parser Example ] for alignment in pybam.read('/my/data.bam'): print alignment.sam_seq </> PybamWarn \u2014 Common base class for all non-exit exceptions. </> PybamError \u2014 Common base class for all non-exit exceptions. </>","title":"ecodam_py.pybam"},{"location":"api/ecodam_py.ml_pipeline/","text":"module ecodam_py . ml_pipeline </> Classes EcoDamData \u2014 EcoDamData(chrom, naked, theo, nfr) </> class ecodam_py.ml_pipeline . EcoDamData ( chrom , naked , theo , nfr ) </> Bases tuple EcoDamData(chrom, naked, theo, nfr)","title":"ecodam_py.ml_pipeline"},{"location":"api/ecodam_py.ml_pipeline/#ecodam_pyml_pipeline","text":"</> Classes EcoDamData \u2014 EcoDamData(chrom, naked, theo, nfr) </> class","title":"ecodam_py.ml_pipeline"},{"location":"api/ecodam_py.ml_pipeline/#ecodam_pyml_pipelineecodamdata","text":"</> Bases tuple EcoDamData(chrom, naked, theo, nfr)","title":"ecodam_py.ml_pipeline.EcoDamData"},{"location":"api/ecodam_py.parse_gene_coding/","text":"module ecodam_py . parse_gene_coding </> A filteration script written pretty specifically for some GTF file that Gil needed to be parsed and filtered. It takes about an hour or two to run on the Irys server due to the slow iteration step that is performed internally by the gffutils library. It also reverses the start and end loci on the rows with the - strand, as seen in the find_minus_primer function.","title":"ecodam_py.parse_gene_coding"},{"location":"api/ecodam_py.parse_gene_coding/#ecodam_pyparse_gene_coding","text":"</> A filteration script written pretty specifically for some GTF file that Gil needed to be parsed and filtered. It takes about an hour or two to run on the Irys server due to the slow iteration step that is performed internally by the gffutils library. It also reverses the start and end loci on the rows with the - strand, as seen in the find_minus_primer function.","title":"ecodam_py.parse_gene_coding"},{"location":"api/ecodam_py.peak_calling/","text":"module ecodam_py . peak_calling </> Functions accompanying the 'peak_calling' notebook. Functions color_peak_surroundings ( atac_split , atac_indices , eco_split , eco_indices ) \u2014 Generate a plot for the peak-centered BedGraphFile data. </> define_peak_surroundings ( atac , atac_peak_indices , eco , eco_peak_indices ) \u2014 Aggregator function to prettify notebook's code. </> preprocess_data ( fname ) (BedGraphFile) \u2014 A series of simple steps that should be done automatically to BedGraphFile data. </> function ecodam_py.peak_calling . preprocess_data ( fname ) </> A series of simple steps that should be done automatically to BedGraphFile data. Parameters fname (pathlib.Path) \u2014 BedGraphFile filename function ecodam_py.peak_calling . define_peak_surroundings ( atac , atac_peak_indices , eco , eco_peak_indices ) </> Aggregator function to prettify notebook's code. The function merely calls a different one for all relevant data that should be displayed. function ecodam_py.peak_calling . color_peak_surroundings ( atac_split , atac_indices , eco_split , eco_indices ) </> Generate a plot for the peak-centered BedGraphFile data. The resulting plot is a barplot that overlays the Eco and ATAC data on the same coordinates.","title":"ecodam_py.peak_calling"},{"location":"api/ecodam_py.peak_calling/#ecodam_pypeak_calling","text":"</> Functions accompanying the 'peak_calling' notebook. Functions color_peak_surroundings ( atac_split , atac_indices , eco_split , eco_indices ) \u2014 Generate a plot for the peak-centered BedGraphFile data. </> define_peak_surroundings ( atac , atac_peak_indices , eco , eco_peak_indices ) \u2014 Aggregator function to prettify notebook's code. </> preprocess_data ( fname ) (BedGraphFile) \u2014 A series of simple steps that should be done automatically to BedGraphFile data. </> function","title":"ecodam_py.peak_calling"},{"location":"api/ecodam_py.peak_calling/#ecodam_pypeak_callingpreprocess_data","text":"</> A series of simple steps that should be done automatically to BedGraphFile data. Parameters fname (pathlib.Path) \u2014 BedGraphFile filename function","title":"ecodam_py.peak_calling.preprocess_data"},{"location":"api/ecodam_py.peak_calling/#ecodam_pypeak_callingdefine_peak_surroundings","text":"</> Aggregator function to prettify notebook's code. The function merely calls a different one for all relevant data that should be displayed. function","title":"ecodam_py.peak_calling.define_peak_surroundings"},{"location":"api/ecodam_py.peak_calling/#ecodam_pypeak_callingcolor_peak_surroundings","text":"</> Generate a plot for the peak-centered BedGraphFile data. The resulting plot is a barplot that overlays the Eco and ATAC data on the same coordinates.","title":"ecodam_py.peak_calling.color_peak_surroundings"},{"location":"api/ecodam_py.pybam/","text":"module ecodam_py . pybam </> Awesome people who have directly contributed to the project: Jon Palmer - Bug finder & advice on project direction Mahmut Uludag - Bug finder Help: print pybam.wat Github: http://github.com/JohnLonginotto/pybam This code was written by John Longinotto, a PhD student of the Pospisilik Lab at the Max Planck Institute of Immunbiology & Epigenetics, Freiburg. My PhD is funded by the Deutsches Epigenom Programm (DEEP), and the Max Planck IMPRS Program. I study Adipose Biology and Circadian Rhythm in mice, although it seems these days I spend most of my time at the computer :-) Classes read \u2014 [ Dynamic Parser Example ] for alignment in pybam.read('/my/data.bam'): print alignment.sam_seq </> PybamWarn \u2014 Common base class for all non-exit exceptions. </> PybamError \u2014 Common base class for all non-exit exceptions. </> class ecodam_py.pybam . read ( f , fields=False , decompressor=False ) </> [ Dynamic Parser Example ] for alignment in pybam.read('/my/data.bam'): print alignment.sam_seq [ Static Parser Example ] for seq,mapq in pybam.read('/my/data.bam',['sam_seq','sam_mapq']): print seq print mapq [ Mixed Parser Example ] my_bam = pybam.read('/my/data.bam',['sam_seq','sam_mapq']) print my_bam._static_parser_code for seq,mapq in my_bam: if seq.startswith('ACGT') and mapq > 10: print my_bam.sam [ Custom Decompressor (from file path) Example ] my_bam = pybam.read('/my/data.bam.lzma',decompressor='lzma --decompress --stdout /my/data.bam.lzma') [ Custom Decompressor (from file object) Example ] my_bam = pybam.read(sys.stdin,decompressor='lzma --decompress --stdout') # data given to lzma via stdin [ Force Internal bgzip Decompressor ] my_bam = pybam.read('/my/data.bam',decompressor='internal') \"print pybam.wat\" in the python terminal to see the possible parsable values, or visit http://github.com/JohnLonginotto/pybam for the latest info. class ecodam_py.pybam . PybamWarn ( ) </> Bases Exception BaseException Common base class for all non-exit exceptions. class ecodam_py.pybam . PybamError ( ) </> Bases Exception BaseException Common base class for all non-exit exceptions.","title":"ecodam_py.pybam"},{"location":"api/ecodam_py.pybam/#ecodam_pypybam","text":"</> Awesome people who have directly contributed to the project: Jon Palmer - Bug finder & advice on project direction Mahmut Uludag - Bug finder Help: print pybam.wat Github: http://github.com/JohnLonginotto/pybam This code was written by John Longinotto, a PhD student of the Pospisilik Lab at the Max Planck Institute of Immunbiology & Epigenetics, Freiburg. My PhD is funded by the Deutsches Epigenom Programm (DEEP), and the Max Planck IMPRS Program. I study Adipose Biology and Circadian Rhythm in mice, although it seems these days I spend most of my time at the computer :-) Classes read \u2014 [ Dynamic Parser Example ] for alignment in pybam.read('/my/data.bam'): print alignment.sam_seq </> PybamWarn \u2014 Common base class for all non-exit exceptions. </> PybamError \u2014 Common base class for all non-exit exceptions. </> class","title":"ecodam_py.pybam"},{"location":"api/ecodam_py.pybam/#ecodam_pypybamread","text":"</> [ Dynamic Parser Example ] for alignment in pybam.read('/my/data.bam'): print alignment.sam_seq [ Static Parser Example ] for seq,mapq in pybam.read('/my/data.bam',['sam_seq','sam_mapq']): print seq print mapq [ Mixed Parser Example ] my_bam = pybam.read('/my/data.bam',['sam_seq','sam_mapq']) print my_bam._static_parser_code for seq,mapq in my_bam: if seq.startswith('ACGT') and mapq > 10: print my_bam.sam [ Custom Decompressor (from file path) Example ] my_bam = pybam.read('/my/data.bam.lzma',decompressor='lzma --decompress --stdout /my/data.bam.lzma') [ Custom Decompressor (from file object) Example ] my_bam = pybam.read(sys.stdin,decompressor='lzma --decompress --stdout') # data given to lzma via stdin [ Force Internal bgzip Decompressor ] my_bam = pybam.read('/my/data.bam',decompressor='internal') \"print pybam.wat\" in the python terminal to see the possible parsable values, or visit http://github.com/JohnLonginotto/pybam for the latest info. class","title":"ecodam_py.pybam.read"},{"location":"api/ecodam_py.pybam/#ecodam_pypybampybamwarn","text":"</> Bases Exception BaseException Common base class for all non-exit exceptions. class","title":"ecodam_py.pybam.PybamWarn"},{"location":"api/ecodam_py.pybam/#ecodam_pypybampybamerror","text":"</> Bases Exception BaseException Common base class for all non-exit exceptions.","title":"ecodam_py.pybam.PybamError"},{"location":"api/source/ecodam_py.bedgraph/","text":"SOURCE CODE ecodam_py. bedgraph DOCS \"\"\"This module provides the basic building blocks for working with BedGraphs\"\"\" import warnings import pathlib from collections import namedtuple import multiprocessing from typing import Optional , Tuple , List , Iterable import pybedtools import numpy as np import pandas as pd import xarray as xr import numba @pd . api . extensions . register_dataframe_accessor ( \"bg\" ) class BedGraphAccessor : DOCS \"\"\" Introduces a `.bg` accessor to DataFrame which provides unique capabilities for the DF, including new methods and properties. To use them, simply import this class to your current scope. \"\"\" def __init__ ( self , pandas_obj ): self . _validate ( pandas_obj ) self . _obj = pandas_obj @staticmethod def _validate ( obj ): \"\"\"verify there is an intensity and a 'chr' column\"\"\" if \"intensity\" not in obj . columns or \"chr\" not in obj . columns : raise AttributeError ( \"Must have all needed columns\" ) if not isinstance ( obj . index , pd . IntervalIndex ): if \"start_locus\" not in obj . columns and \"end_locus\" not in obj . columns : raise AttributeError ( \"Index and columns not in BedGraph format\" ) else : assert obj . index . name == \"locus\" def index_to_columns ( self ) -> pd . DataFrame : DOCS \"\"\"Encodes the loci information in two columns of the DF. This method takes a DF that has its locus information encoded in its index, possibly as a result of calling [columns_to_index](ecodam_py.bedgraph.BedGraphAccessor.columns_to_index) on it, and changes it so that its index becomes a standard RangeIndex and the loci infomration is kept in two `np.int64` columns, 'start_locus' and 'end_locus'. Returns ------- A modified DF with two new columns for the locus info \"\"\" if \"start_locus\" in self . _obj . columns and \"end_locus\" in self . _obj . columns : return self . _obj obj = self . _obj . copy () obj . loc [:, \"start_locus\" ] = obj . index . left obj . loc [:, \"end_locus\" ] = obj . index . right obj = obj . reset_index () . drop ( \"locus\" , axis = 1 )[[ \"chr\" , \"start_locus\" , \"end_locus\" , \"intensity\" ]] return obj def columns_to_index ( self ) -> pd . DataFrame : DOCS \"\"\"Encodes the loci information in the index of the DF. This method takes a DF that has its locus information as columns, i.e. 'start_locus' and 'end_locus', and moves these columns to a new pd.IntervalIndex instead. Returns ------- pd.DataFrame A new DF with an IntervalIndex instead of the 'start_locus' and 'end_locus' columns \"\"\" if self . _obj . index . name == \"locus\" : return self . _obj obj = self . _obj . copy () obj . loc [:, \"locus\" ] = pd . IntervalIndex . from_arrays ( obj . loc [:, \"start_locus\" ], obj . loc [:, \"end_locus\" ], closed = \"left\" , name = \"locus\" , ) obj = obj . set_index ( \"locus\" ) . drop ([ \"start_locus\" , \"end_locus\" ], axis = 1 ) return obj def add_chr ( self , chr_ : str = \"chr15\" ) -> pd . DataFrame : DOCS \"\"\"Adds a 'chr' column to a copied DF. Useful in certain internal situations mostly. Parameters ---------- chr : str, optional The chromosome name to add Returns ------- pd.DataFrame A new DF with the new column \"\"\" obj = self . _obj . copy () if \"chr\" in obj . columns : obj = obj . astype ({ \"chr\" : \"category\" }) return obj obj . loc [:, \"chr\" ] = chr_ obj = obj . astype ({ \"chr\" : \"category\" }) return obj def serialize ( self , fname : pathlib . Path , mode : str = \"w\" ): DOCS \"\"\"Writes the BedGraph to disk. This method first normalizes the BedGraph and then writes it to disk with the given write mode. Parameters ---------- fname : pathlib.Path Filename to write to mode : str File writing mode (similar to the built-in open()). By default 'w' \"\"\" self . index_to_columns () . reindex ( [ \"chr\" , \"start_locus\" , \"end_locus\" , \"intensity\" ], axis = 1 ) . to_csv ( fname , sep = \" \\t \" , header = None , index = False , mode = mode ) def unweighted_overlap ( DOCS self , other : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Find the overlapping parts of self and other and return these areas. This method looks for overlapping parts of the two BedGraph DFs and returns only the relevant slices of these two objects. The overlap is called 'unweighted' because it considers an area to be overlapping even if a single BP overlaps between the two loci. This might seem odd, but it's helpful since we usually work with BedGraph files that have the same coordinates, so this method is good enough. Use the (slower) 'weighted_overlap' method if you need to assert that the overlap is signifanct in terms of BP counts. Parameters ---------- other : pd.DataFrame A BedGraph DF Returns ------- (pd.DataFrame, pd.DataFrame) The 'self' and 'other' DFs only at the rows that overlap \"\"\" obj = self . columns_to_index () other = other . bg . columns_to_index () selfidx , otheridx = [], [] for idx , interval in enumerate ( obj . index ): overlapping_idx = np . where ( other . index . overlaps ( interval ))[ 0 ] if overlapping_idx . size > 0 : otheridx . append ( overlapping_idx [ 0 ]) selfidx . append ( idx ) return obj . iloc [ selfidx ], other . iloc [ otheridx ] def weighted_overlap ( DOCS self , other : pd . DataFrame , overlap_pct : float = 0.75 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Find the overlapping parts of the two DFs with at least 'overlap_pct' amount of overlap. In the first phase of this method, the two DFs are put on the same coordinates so that they could be compared in a viable manner. Currently the way its done is to move them to 1bp resolution which automatically assists in these types of calculations. The 1bp resolution data is masked data, i.e. the 'intensity' values can only be 0 or 1. Then the DFs are multiplied and grouped by their previous starts and ends, i.e. each group is now a specified loci in the original data. Using a groupby operation and a mean calculation we see which group's average is higher than the given 'overlap_pct' value, and if it is we mark that group as overlapping. Parameters ---------- other : pd.DataFrame A DF with the relevant data overlap_pct : float, optional The percentage of loci that should overlap between the two datasets Returns ------- (pd.DataFrame, pd.DataFrame) Only the overlapping areas from the self and other DFs \"\"\" equalized_this , equalized_other = equalize_loci ( self . _obj . copy (), other . copy ()) unified = equalized_this . at_1bp * equalized_other . at_1bp means = ( pd . DataFrame ({ \"group\" : equalized_other . groups , \"unified\" : unified }) . groupby ( \"group\" ) . mean () ) means = means . drop ( index =- 1 , errors = \"ignore\" ) assert len ( means ) == len ( equalized_other . even ) means = means . query ( \"unified > @overlap_pct\" ) other_result = equalized_other . even . loc [ means . index , :] means = ( pd . DataFrame ({ \"group\" : equalized_this . groups , \"unified\" : unified }) . groupby ( \"group\" ) . mean () ) means = means . drop ( index =- 1 , errors = \"ignore\" ) assert len ( means ) == len ( equalized_this . even ) means = means . query ( \"unified > @overlap_pct\" ) self_result = equalized_this . even . loc [ means . index , :] return self_result , other_result def to_1bp_resolution ( self , multi_chrom = True ) -> pd . DataFrame : DOCS \"\"\"Changes the coordinates of the given DF to have 1bp resolution.\"\"\" obj = self . index_to_columns () if multi_chrom : grouped = obj . groupby ( \"chr\" , as_index = False ) with multiprocessing . Pool () as pool : result = pool . starmap ( self . _single_chr_to_1bp , grouped ) return pd . concat ( result , ignore_index = False , axis = 0 ) else : return self . _single_chr_to_1bp ( \"\" , obj ) @staticmethod def _single_chr_to_1bp ( chr_group : str , obj : pd . DataFrame ) -> pd . DataFrame : _ , groups = intervals_to_1bp_mask ( obj . start_locus . to_numpy () . copy (), obj . end_locus . to_numpy () . copy () ) starts = np . arange ( obj . loc [:, \"start_locus\" ] . iloc [ 0 ], obj . loc [:, \"end_locus\" ] . iloc [ - 1 ] ) ends = starts + 1 chr_ = obj . loc [:, \"chr\" ] . iloc [ groups - 1 ] intensity = obj . loc [:, \"intensity\" ] . iloc [ groups - 1 ] return pd . DataFrame ( { \"chr\" : chr_ , \"start_locus\" : starts , \"end_locus\" : ends , \"intensity\" : intensity , } ) . astype ({ \"chr\" : \"category\" }) def pad_with_zeros ( nfr : pd . DataFrame , chrom : pd . DataFrame ): DOCS \"\"\"Adds zero entries for loci which are not included in one of the given DFs\"\"\" if nfr . start_locus . iloc [ 0 ] < chrom . start_locus . iloc [ 0 ]: dup = chrom . iloc [ 0 ] . copy () dup . start_locus = nfr . start_locus . iloc [ 0 ] dup . end_locus = chrom . start_locus . iloc [ 0 ] dup . intensity = 0 chrom = pd . concat ([ dup . to_frame () . T , chrom ], axis = 0 , ignore_index = True ) . astype ( { \"start_locus\" : np . uint64 , \"end_locus\" : np . uint64 } ) elif nfr . start_locus . iloc [ 0 ] > chrom . start_locus . iloc [ 0 ]: dup = nfr . iloc [ 0 ] . copy () dup . start_locus = chrom . start_locus . iloc [ 0 ] dup . end_locus = nfr . start_locus . iloc [ 0 ] dup . intensity = 0 nfr = pd . concat ([ dup . to_frame () . T , nfr ], axis = 0 , ignore_index = True ) . astype ( { \"start_locus\" : np . uint64 , \"end_locus\" : np . uint64 } ) if nfr . end_locus . iloc [ - 1 ] < chrom . end_locus . iloc [ - 1 ]: dup = nfr . iloc [ - 1 ] . copy () dup . start_locus = nfr . end_locus . iloc [ - 1 ] dup . end_locus = chrom . end_locus . iloc [ - 1 ] dup . intensity = 0 nfr = pd . concat ([ nfr , dup . to_frame () . T ], axis = 0 , ignore_index = True ) . astype ( { \"start_locus\" : np . uint64 , \"end_locus\" : np . uint64 } ) elif nfr . end_locus . iloc [ - 1 ] > chrom . end_locus . iloc [ - 1 ]: dup = chrom . iloc [ - 1 ] . copy () dup . start_locus = chrom . end_locus . iloc [ - 1 ] dup . end_locus = nfr . end_locus . iloc [ - 1 ] dup . intensity = 0 chrom = pd . concat ([ chrom , dup . to_frame () . T ], axis = 0 , ignore_index = True ) . astype ( { \"start_locus\" : np . uint64 , \"end_locus\" : np . uint64 } ) return nfr , chrom @numba . njit ( \"Tuple((u1[:], i8[:]))(u8[:], u8[:], i8[:])\" , cache = True , parallel = False ) DOCS def intervals_to_1bp_mask ( start : np . ndarray , end : np . ndarray , orig_groups : np . ndarray , ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\"Generate a new 1bp BedGraph and keep information of the original distribution and sources of data. \"\"\" length = end [ - 1 ] - start [ 0 ] mask = np . zeros ( length , dtype = np . uint8 ) groups = np . full ( length , - 1 , dtype = np . int64 ) end -= start [ 0 ] start -= start [ 0 ] one = np . uint8 ( 1 ) for idx in numba . prange ( len ( start )): st = start [ idx ] en = end [ idx ] mask [ st : en ] = one groups [ st : en ] = orig_groups [ idx ] return mask , groups def _trim_start_end ( data : pd . DataFrame , start : int , end : int ): \"\"\"Cuts the data so that it starts at start and ends at end. The values refer to the 'start_locus' column of the data DataFrame. Parameters ---------- data : pd.DataFrame Data before trimming start, end : int Values from 'start_locus' to cut by Returns ------- pd.DataFrame Trimmed data \"\"\" start_idx = data . loc [:, \"start_locus\" ] . searchsorted ( start ) end_idx = data . loc [:, \"start_locus\" ] . searchsorted ( end , side = \"left\" ) return data . iloc [ start_idx : end_idx , :] def put_dfs_on_even_grounds ( dfs : Iterable [ pd . DataFrame ]) -> Iterable [ pd . DataFrame ]: DOCS \"\"\"Asserts overlap of all given DataFrames. An accompanying function to 'put_on_even_grounds' that does the heavy lifting. \"\"\" dfs = list ( dfs ) starts = ( data . start_locus . iloc [ 0 ] for data in dfs ) unified_start = max ( starts ) ends = ( data . end_locus . iloc [ - 1 ] for data in dfs ) unified_end = min ( ends ) new_dfs = ( _trim_start_end ( data , unified_start , unified_end ) for data in dfs ) return new_dfs #: A unique return type for equalize_loci Equalized = namedtuple ( \"Equalized\" , \"even at_1bp groups\" ) def equalize_loci ( DOCS first : pd . DataFrame , second : pd . DataFrame ) -> Tuple [ Equalized , Equalized ]: \"\"\" Generate an approximately-equal loci DFs from the two given ones. This function is a mini-pipeline designed to bring two DFs to have the same loci. It works great with a small caveat - the returned 'even' DFs might have 'holes' that aren't covered by any loci. In other words, the loci of the DFs don't necessarily populate all of the loci from the starting point to the ending one. Again, this is only relevant to the 'even' entry, the 1bp resolution copy obviously does cover all BP. Parameters ---------- first, second : pd.DataFrame Two DFs that have to be equalized Returns ------- (Equalized, Equalized) An even variant (same as the original but only with relevant loci) and 1 bp resolution version with the groups (original loci) marked Raises ------ RuntimeError If the DFs cannot be coerced to have the same loci \"\"\" first = first . sort_values ( 'start_locus' ) first = first . drop_duplicates ([ 'start_locus' , 'end_locus' ]) . astype ({ 'start_locus' : np . uint64 , 'end_locus' : np . uint64 }) second = second . sort_values ( 'start_locus' ) second = second . drop_duplicates ([ 'start_locus' , 'end_locus' ]) . astype ({ 'start_locus' : np . uint64 , 'end_locus' : np . uint64 }) self_even , other_even = put_dfs_on_even_grounds ([ first , second ]) if len ( self_even ) == 0 or len ( other_even ) == 0 : raise RuntimeError ( \"No intersection possible between the two DFs\" ) self_even , other_even = pad_with_zeros ( self_even , other_even ) self_at_1bp , self_groups = intervals_to_1bp_mask ( self_even . start_locus . to_numpy (), self_even . end_locus . to_numpy (), self_even . index . to_numpy () . astype ( np . int64 ), ) other_at_1bp , other_groups = intervals_to_1bp_mask ( other_even . start_locus . to_numpy (), other_even . end_locus . to_numpy (), other_even . index . to_numpy () . astype ( np . int64 ), ) return Equalized ( self_even , self_at_1bp , self_groups ), Equalized ( other_even , other_at_1bp , other_groups ) class BedGraphFile : DOCS def __init__ ( self , file : pathlib . Path , header = True ): \"\"\"A BedGraphFile file which can be manipulated an displayed. The init function will also normalize column names for easier processing down the pipeline. Parameters ---------- file : pathlib.Path Data as BedGraph to read header : bool, optional Whether the file contains a header or not \"\"\" warnings . warn ( \"This class is deprecated and will be removed in a future version. Please use the BedGraphAccessor class.\" , DeprecationWarning , ) self . file = file if header : self . data = pd . read_csv ( file , sep = \" \\t \" ) self . data . columns = self . data . columns . str . replace ( \" \" , \"_\" ) . str . lower () self . _sort_molecule_by_intensity () self . data = self . data . astype ({ \"molid\" : \"category\" }) else : self . data = pd . read_csv ( file , sep = \" \\t \" , header = None , names = [ \"chr\" , \"start_locus\" , \"end_locus\" , \"intensity\" ], ) self . data = self . data . astype ({ \"chr\" : \"category\" }) def add_center_locus ( self ): DOCS \"\"\"Adds a center point to each segment of a molecule. This is done so that the intensity value can be assigned to a specific base for easier processing. \"\"\" self . data . loc [:, \"center_locus\" ] = ( self . data . loc [:, \"start_locus\" ] + self . data . loc [:, \"end_locus\" ] ) / 2 def _sort_molecule_by_intensity ( self ): \"\"\"Sorts the DF so that the first molecules are the dimmest, and the last ones are the brightest\"\"\" sorted_molids = list ( self . data . groupby ( \"molid\" )[ \"intensity\" ] . mean () . sort_values () . to_dict () . keys () ) sorter = dict ( zip ( sorted_molids , np . arange ( len ( sorted_molids )))) self . data . loc [:, \"molid_rank\" ] = self . data [ \"molid\" ] . map ( sorter ) self . data = self . data . sort_values ( \"molid_rank\" ) def convert_df_to_da ( self ): DOCS \"\"\"Create a more complex representation of the tracks as an xr.DataArray. The array is a two dimensional array with coordinates that denote the molecule name and its brightness at any given locus. The method creates a new self.dataarray attribute which contains this DataArray. \"\"\" mol_ids = self . data [ \"molid\" ] . unique () . astype ( \"<U8\" ) molnum = np . arange ( len ( mol_ids )) loci = np . unique ( np . concatenate ( [ self . data [ \"start_locus\" ] . unique (), self . data [ \"end_locus\" ] . unique ()] ) ) loci . sort () coords = { \"locus\" : loci , \"molnum\" : molnum , \"molid\" : ( \"molnum\" , mol_ids )} attrs = { \"chr\" : self . data . loc [ 0 , \"chromosome\" ]} dims = [ \"molnum\" , \"locus\" ] da = xr . DataArray ( np . full (( len ( mol_ids ), len ( loci )), np . nan ), dims = dims , coords = coords , attrs = attrs , ) if np . asarray ( self . data . intensity . unique () == np . array ([ 0 , 1 ])) . all (): self . dataarray = self . _populate_da_with_thresholded_intensity ( da ) else : self . dataarray = self . _populate_da_with_intensity ( da ) def _populate_da_with_thresholded_intensity ( self , da : xr . DataArray ): \"\"\"Iterate over the initial DA and populate an array with its intensity values, assuming they're only 0 and 1. The method also normalizes the intensity counts so that the recorded value is the average of the new value and the previous one. There's a slight issue with this normalization step since the data is zeroed before any actual calculations are done. This means that if some part of a molecule already had a non-NaN value, it's zeroed out. From my current understanding the chances for this are zero, but perhaps I'm missing something. The underlying DataFrame contains only 0's and 1's so we skip the normalization step that occurs in the sister method. \"\"\" for row in self . data . itertuples ( index = False ): sl = ( slice ( row . molid_rank , row . molid_rank + 1 ), slice ( row . start_locus , row . end_locus ), ) da . loc [ sl ] = row . intensity return da def _populate_da_with_intensity ( self , da : xr . DataArray ): \"\"\"Iterate over the initial DA and populate an array with its intensity values. The method also normalizes the intensity counts so that the recorded value is the average of the new value and the previous one. There's a slight issue with this normalization step since the data is zeroed before any actual calculations are done. This means that if some part of a molecule already had a non-NaN value, it's zeroed out. From my current understanding the chances for this are zero, but perhaps I'm missing something. \"\"\" da_norm_counts = da . copy () da_norm_counts [:] = 1 for row in self . data . itertuples ( index = False ): sl = ( slice ( row . molid_rank , row . molid_rank + 1 ), slice ( row . start_locus , row . end_locus ), ) current_data = da . loc [ sl ] current_nans = np . isnan ( current_data ) if np . any ( current_nans ): current_data [:] = 0 da . loc [ sl ] = ( current_data + row . intensity ) / da_norm_counts . loc [ sl ] da_norm_counts . loc [ sl ] += 1 return da def smooth ( self , window : int = 1000 ): DOCS \"\"\"Smooths out the data with the given-sized window. Parameters ---------- window : int Smooth data by this window \"\"\" weights = np . ones ( window ) weights /= weights . sum () self . data [ \"smoothed\" ] = np . convolve ( self . data [ \"intensity\" ], weights , mode = \"same\" ) if __name__ == \"__main__\" : nfr_all_chrom = pathlib . Path ( \"/mnt/saphyr/Saphyr_Data/DAM_DLE_VHL_DLE/Hagai/ENCFF240YRV.sorted.bedgraph\" )","title":"ecodam_py.bedgraph"},{"location":"api/source/ecodam_py.compare_ecodam_atac/","text":"SOURCE CODE ecodam_py. compare_ecodam_atac DOCS import pathlib import copy from typing import Tuple , List import warnings import numpy as np import pandas as pd import skimage.exposure import matplotlib.pyplot as plt import scipy.signal from ecodam_py.bedgraph import BedGraphFile def _trim_start_end ( data : pd . DataFrame , start : int , end : int ): \"\"\"Cuts the data so that it starts at start and ends at end. The values refer to the 'start_locus' column of the data DataFrame. Parameters ---------- data : pd.DataFrame Data before trimming start, end : int Values from 'start_locus' to cut by Returns ------- pd.DataFrame Trimmed data \"\"\" start_idx = data . loc [:, \"start_locus\" ] . searchsorted ( start ) end_idx = data . loc [:, \"start_locus\" ] . searchsorted ( end , side = \"right\" ) return data . iloc [ start_idx : end_idx , :] def put_on_even_grounds ( beds : List [ BedGraphFile ]) -> List [ BedGraphFile ]: DOCS \"\"\"Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. The function will trim the datasets but will try to leave as many base pairs as possible. Parameters ---------- beds : List[BedGraphFile] Data to trim in the order Eco, ATAC, Naked Returns ------- List[BedGraphFile] \"\"\" starts = [ bed . data . iloc [ 0 , 1 ] for bed in beds ] unified_start = max ( starts ) ends = [ bed . data . iloc [ - 1 , 1 ] for bed in beds ] unified_end = min ( ends ) new_dfs = [ _trim_start_end ( bed . data , unified_start , unified_end ) for bed in beds ] for bed , new_df in zip ( beds , new_dfs ): bed . data = new_df return beds def convert_to_intervalindex ( beds : List [ BedGraphFile ]) -> List [ BedGraphFile ]: warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) for bed in beds : left = bed . data . loc [:, \"start_locus\" ] . copy () right = bed . data . loc [:, \"end_locus\" ] . copy () data = bed . data . drop ([ \"start_locus\" , \"end_locus\" ], axis = 1 ) index = pd . IntervalIndex . from_arrays ( left , right , closed = \"left\" , name = \"locus\" ) data = data . set_index ( index ) bed . data = data return beds def generate_intervals_1kb ( data ) -> pd . IntervalIndex : warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) first , last = data . index [ 0 ], data . index [ - 1 ] idx = pd . interval_range ( first . left , last . right , freq = 1000 , closed = \"left\" ) return idx def equalize_distribs ( eco , naked , atac ): warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) eco = normalize_df_between_01 ( eco ) naked = normalize_df_between_01 ( naked ) max_ = atac . intensity . max () min_ = atac . intensity . min () eco . loc [:, \"intensity\" ] = ( eco . loc [:, \"intensity\" ] * ( max_ - min_ )) + min_ naked . loc [:, \"intensity\" ] = ( naked . loc [:, \"intensity\" ] * ( max_ - min_ )) + min_ return eco , naked def normalize_df_between_01 ( data ): warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) data . loc [:, \"intensity\" ] -= data . loc [:, \"intensity\" ] . min () data . loc [:, \"intensity\" ] /= data . loc [:, \"intensity\" ] . max () return data def match_histograms ( eco : pd . DataFrame , atac : pd . DataFrame ): warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) atac_matched = skimage . exposure . match_histograms ( atac . intensity . to_numpy (), eco . intensity . to_numpy () ) return atac_matched def plot_bg ( eco , naked , atac ): fig , ax = plt . subplots () ax . plot ( eco . index . mid , eco . iloc [:, 0 ], label = \"EcoDAM\" , alpha = 0.25 ) ax . plot ( naked . index . mid , naked . iloc [:, 0 ], label = \"Naked\" , alpha = 0.25 ) ax . plot ( atac . index . mid , atac . iloc [:, 0 ], label = \"ATAC\" , alpha = 0.25 ) ax . legend () ax . set_ylabel ( \"Intensity\" ) ax . set_xlabel ( \"Locus\" ) return ax def find_closest_diff ( eco , atac , thresh = 0.5 ): diff = np . abs ( eco . to_numpy () . ravel () - atac . to_numpy () . ravel ()) closest = diff < thresh closest_eco = eco . loc [ closest , :] closest_atac = newatac . loc [ closest , :] return closest_eco , closest_atac def write_intindex_to_disk ( data : pd . DataFrame , fname : pathlib . Path , chr_ : str = 'chr15' ): warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) start = data . index . left . copy () end = data . index . right . copy () data . loc [:, \"start\" ] = start data . loc [:, \"end\" ] = end data . loc [:, \"chr\" ] = chr_ data = data . reindex ([ \"chr\" , \"start\" , \"end\" , \"intensity\" ], axis = 1 ) data . to_csv ( fname , sep = \" \\t \" , header = None , index = False ) def find_clusters ( data : pd . DataFrame , cluster_thresh = 15_000 ) -> pd . DataFrame : shoulders = 5000 diffs = data . start . iloc [ 1 :] . to_numpy () - data . end . iloc [: - 1 ] . to_numpy () diffs = np . concatenate ([[ 0 ], diffs ]) clusters = diffs < cluster_thresh cluster_end_idx = np . where ( ~ clusters )[ 0 ] cluster_start = 0 cluster_data = pd . DataFrame ( np . zeros (( len ( cluster_end_idx ), 2 ), dtype = np . int64 ), columns = [ \"start\" , \"end\" ] ) for row , end in enumerate ( cluster_end_idx ): cluster = data . iloc [ cluster_start : end , :] cluster_start = end locus_start = cluster . iloc [ 0 , 1 ] - shoulders locus_end = cluster . iloc [ - 1 , 2 ] + shoulders cluster_data . iloc [ row , :] = locus_start , locus_end return cluster_data def concat_clusters ( eco : pd . DataFrame , atac : pd . DataFrame , clusters : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: columns = [ \"intensity\" ] clustered_eco = pd . DataFrame ( columns = columns ) clustered_atac = pd . DataFrame ( columns = columns ) for _ , clust in clusters . iterrows (): current_eco = eco . loc [ clust . start : clust . end ] current_atac = atac . loc [ clust . start : clust . end ] clustered_eco = clustered_eco . append ( current_eco ) clustered_atac = clustered_atac . append ( current_atac ) return clustered_eco , clustered_atac def normalize_eco_with_naked ( eco , naked ): naked_normed = normalize_df_between_01 ( naked ) eco_normed = eco / naked_normed return eco_normed def preprocess_bedgraph ( paths : List [ pathlib . Path ]) -> List [ BedGraphFile ]: warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) res = [] for path in paths : bed = BedGraphFile ( path , header = False ) bed . data = bed . data . sort_values ( \"start_locus\" ) res . append ( copy . deepcopy ( bed )) return res def normalize_naked_with_theo ( naked : pd . DataFrame , theo : pd . DataFrame ) -> pd . DataFrame : warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) no_sites = theo . intensity == 0 zero_distrib = naked . loc [ no_sites ] zero_distrib . hist () plt . show () baseline_intensity = zero_distrib . mean () . to_numpy ()[ 0 ] naked . loc [:, 'intensity' ] = naked . loc [:, 'intensity' ] . clip ( lower = baseline_intensity ) - baseline_intensity # relevant_theo = theo.loc[~no_sites] return naked def preprocess_theo ( fname : pathlib . Path , chr_ : str = 'chr15' ): warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) bed = BedGraphFile ( fname , header = False ) bed . data = bed . data . query ( 'chr == @chr_' ) bed . data = bed . data . sort_values ( 'start_locus' ) bed = convert_to_intervalindex ([ bed ])[ 0 ] return bed def reindex_theo_data ( naked : pd . DataFrame , theo : pd . DataFrame ) -> pd . DataFrame : warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) new_theo = pd . DataFrame ({ 'chr' : 'chr15' , 'intensity' : np . zeros ( len ( naked ), dtype = np . float64 )}) for idx in range ( len ( new_theo )): new_theo . iloc [ idx , - 1 ] = theo . loc [ naked . index [ idx ] . left : naked . index [ idx ] . right ] . intensity . mean () newidx = pd . IntervalIndex . from_arrays ( naked . index . left , naked . index . right , closed = 'left' , name = 'locus' ) new_theo . loc [:, 'locus' ] = newidx new_theo = new_theo . set_index ( 'locus' ) return new_theo if __name__ == \"__main__\" : eco_fname = pathlib . Path ( \"/mnt/saphyr/Saphyr_Data/DAM_DLE_VHL_DLE/Hagai/chromatin_chr15.filter17_60_75.NoBlacklist.NoMask.bedgraph\" ) atac_fname = pathlib . Path ( \"/mnt/saphyr/Saphyr_Data/DAM_DLE_VHL_DLE/Hagai/ATAC_rep1to3_Fold_change_over_control.chr15.bedgraph\" ) naked_fname = pathlib . Path ( \"/mnt/saphyr/Saphyr_Data/DAM_DLE_VHL_DLE/Hagai/NakedAF647_channel2_chr15_NoMissingChromatinWin.BEDgraph\" ) theo_fname = pathlib . Path ( \"/mnt/saphyr/Saphyr_Data/DAM_DLE_VHL_DLE/Hagai/hg38.1kb.windows.InSilico.count.map.sum.bedgraph\" ) beds = preprocess_bedgraph ([ eco_fname , atac_fname , naked_fname ]) theo = preprocess_theo ( theo_fname ) beds [ 1 ] . data . loc [:, \"end_locus\" ] += 100 # ATAC beds = put_on_even_grounds ( beds ) beds = convert_to_intervalindex ( beds ) newint = generate_intervals_1kb ( beds [ 1 ] . data ) newatac = pd . DataFrame ( np . full ( len ( newint ), np . nan ), index = newint , columns = [ \"intensity\" ] ) for int_ in newint : overlapping = beds [ 1 ] . data . index . overlaps ( int_ ) # what if len(overlapping) == 0? newatac . loc [ int_ , \"intensity\" ] = ( beds [ 1 ] . data . loc [ overlapping , \"intensity\" ] . mean () ) newatac = newatac . reindex ( beds [ 0 ] . data . index ) eco = beds [ 0 ] naked = beds [ 2 ] theo . data = reindex_theo_data ( naked . data , theo . data ) naked . data = normalize_naked_with_theo ( naked . data , theo . data ) eco_no_min , naked_no_min = equalize_distribs ( eco . data . drop ( \"chr\" , axis = 1 ), naked . data . drop ( \"chr\" , axis = 1 ), newatac ) ax = plot_bg ( eco_no_min , naked_no_min , newatac ) # eco_normed = normalize_eco_with_naked(eco_no_min, naked_no_min) closest_eco , closest_atac = find_closest_diff ( eco_no_min , newatac ) write_intindex_to_disk ( closest_eco , eco_fname . with_name ( \"chromatin_chr15.filter17_60_75.NoBlacklist.NoMask_top_similarities_with_atac.bedgraph\" ), ) write_intindex_to_disk ( closest_atac , atac_fname . with_name ( \"ATAC_rep1to3_Fold_change_over_control.chr15_top_similarities_with_eco.bedgraph\" ), ) plot_bg ( closest_eco , closest_atac ) eco_samples , eco_welch = scipy . signal . welch ( eco_no_min . dropna () . to_numpy () . ravel ()) atac_samples , atac_welch = scipy . signal . welch ( newatac . dropna () . to_numpy () . ravel ()) fig , axx = plt . subplots () axx . plot ( eco_samples , eco_welch , \"C0\" , alpha = 0.3 ) axx . plot ( atac_samples , atac_welch , \"C1\" , alpha = 0.3 ) clusters = find_clusters ( closest_eco ) clustered_eco , clustered_atac = concat_clusters ( eco_no_min , newatac , clusters ) write_intindex_to_disk ( clustered_eco , eco_fname . with_name ( \"chromatin_chr15.filter17_60_75.NoBlacklist.NoMask_top_similarities_with_atac_after_seeding.bedgraph\" ), ) write_intindex_to_disk ( clustered_atac , atac_fname . with_name ( \"ATAC_rep1to3_Fold_change_over_control.chr15_top_similarities_with_eco_after_seeding.bedgraph\" ), ) plt . show ()","title":"ecodam_py.compare_ecodam_atac"},{"location":"api/source/ecodam_py.eco_atac_normalization/","text":"SOURCE CODE ecodam_py. eco_atac_normalization DOCS \"\"\" Functions accompanying the 'normalize_eco_atac.ipynb' notebook. This is a library-like assortment of functions with no clear start or end, so don't expect any logic that can make it seem ordered. Moreover, some of the functionality exists in more than one place due to the chaotic way in which this project was created. \"\"\" import pathlib import copy from typing import Tuple , List , Union , Iterable import seaborn as sns import numpy as np import numba import pandas as pd import skimage.exposure import matplotlib.pyplot as plt import scipy.stats import scipy.fftpack from ecodam_py.bedgraph import ( BedGraphFile , BedGraphAccessor , put_dfs_on_even_grounds , pad_with_zeros , intervals_to_1bp_mask , ) def read_bedgraph ( fname : pathlib . Path ) -> pd . DataFrame : fname = pathlib . Path ( str ( fname )) assert fname . exists () return ( pd . read_csv ( fname , sep = \" \\t \" , header = None , names = [ \"chr\" , \"start_locus\" , \"end_locus\" , \"intensity\" ], ) . astype ({ \"chr\" : \"category\" , 'start_locus' : np . uint64 , 'end_locus' : np . uint64 }) . dropna () ) def put_on_even_grounds ( beds : List [ BedGraphFile ]) -> List [ BedGraphFile ]: DOCS \"\"\"Makes sure that the Eco, Naked and ATAC data start and end at overlapping areas. The function will trim the datasets but will try to leave as many base pairs as possible. Parameters ---------- beds : List[BedGraphFile] Data to trim in the order Eco, ATAC, Naked Returns ------- List[BedGraphFile] \"\"\" dfs = [ bed . data for bed in beds ] new_dfs = put_dfs_on_even_grounds ( dfs ) for bed , new_df in zip ( beds , new_dfs ): bed . data = new_df return beds def convert_to_intervalindex ( beds : List [ BedGraphFile ]) -> List [ BedGraphFile ]: DOCS \"\"\"Creates an IntervalIndex index for each BedGraphFile in the given list. An IntervalIndex object is very suitable for the task of describing the way tracks are distributed, and this function's goal is to bulk translate the existing information about intensities into these intervals. As a rule of thumb I decided to make the interval closed on the left end. Parameters ---------- beds : List[BedGraphFile] Returns ------- List[BedGraphFile] \"\"\" for bed in beds : left = bed . data . loc [:, \"start_locus\" ] . copy () right = bed . data . loc [:, \"end_locus\" ] . copy () data = bed . data . drop ([ \"start_locus\" , \"end_locus\" ], axis = 1 ) index = pd . IntervalIndex . from_arrays ( left , right , closed = \"left\" , name = \"locus\" ) data = data . set_index ( index ) bed . data = data return beds def generate_intervals_1kb ( data : pd . DataFrame ) -> pd . IntervalIndex : DOCS \"\"\"Creates evenly spaced intervals from existing DFs. The given DF should contain an IntervalIndex and this function is a step in making these intervals evenly spaced and equal to other intervals. Parameters ---------- data : pd.DataFrame A DF of a BedGraphFile file after its index was converted to an IntervalIndex Returns ------- pd.IntervalIndex Evenly spaced IntervalIndex at 1000 bp. \"\"\" warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) first , last = data . index [ 0 ], data . index [ - 1 ] idx = pd . interval_range ( first . left , last . right , freq = 1000 , closed = \"left\" ) return idx def equalize_distribs ( dfs : List [ pd . DataFrame ], atac : pd . DataFrame ): DOCS \"\"\"Change the data limits of the DFs to match the ATAC one. This normalization step helps us compare datasets that are displayed with the same values. \"\"\" warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) dfs = [ normalize_df_between_01 ( df ) for df in dfs ] max_ = atac . intensity . max () min_ = atac . intensity . min () for df in dfs : df . loc [:, \"intensity\" ] = ( df . loc [:, \"intensity\" ] * ( max_ - min_ )) + min_ return dfs def normalize_df_between_01 ( data ): warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) data . loc [:, \"intensity\" ] -= data . loc [:, \"intensity\" ] . min () data . loc [:, \"intensity\" ] /= data . loc [:, \"intensity\" ] . max () return data def match_histograms ( eco : pd . DataFrame , atac : pd . DataFrame ): DOCS \"\"\"Wrapper for the match_histograms scikit-image function\"\"\" warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) atac_matched = skimage . exposure . match_histograms ( atac . intensity . to_numpy (), eco . intensity . to_numpy () ) return atac_matched def plot_bg ( eco , naked , atac ): DOCS \"\"\"Plots a BedGraphFile's DF\"\"\" warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) fig , ax = plt . subplots () ax . plot ( eco . index . mid , eco . iloc [:, 0 ], label = \"EcoDAM\" , alpha = 0.25 ) ax . plot ( naked . index . mid , naked . iloc [:, 0 ], label = \"Naked\" , alpha = 0.25 ) ax . plot ( atac . index . mid , atac . iloc [:, 0 ], label = \"ATAC\" , alpha = 0.25 ) ax . legend () ax . set_ylabel ( \"Intensity\" ) ax . set_xlabel ( \"Locus\" ) return ax def find_closest_diff ( DOCS eco : pd . Series , atac : pd . Series , thresh : float = 0.5 ) -> Tuple [ pd . Series , pd . Series ]: \"\"\"Subtracts the two given tracks and returns them only at the locations closer than thresh. For the given Series objects the function runs an elementwise subtraction and finds the closest areas of the two tracks. Thresh is used as an absolute value and not a relative one. Parameters ---------- eco, atac : pd.Series Two tracks to diff thresh : float The value below which the datasets are considered close Returns ------- Tuple[pd.Series, pd.Series] The two original datasets but only at the locations at which hey match \"\"\" diff = np . abs ( eco . to_numpy () . ravel () - atac . to_numpy () . ravel ()) closest = diff < thresh closest_eco = eco . loc [ closest ] closest_atac = atac . loc [ closest ] return closest_eco , closest_atac def write_intindex_to_disk ( DOCS data : Union [ pd . Series , pd . DataFrame ], fname : pathlib . Path , chr_ : str = \"chr15\" ): \"\"\"Writes the data to disk as a bedgraph. Serializes the data assuming that it has an IntervalIndex as its locus data. The serialization format is basically a bedgraph, i.e. four columns - chr, start, end, intensity. Parameters ---------- data : Union[pd.Series, pd.DataFrame] Bedgraph-like data to serialize fname : pathlib.Path New filename chr_ : str Chromosome \"\"\" warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) data = data . copy () start = data . index . left end = data . index . right try : data = data . to_frame () except AttributeError : pass data . loc [:, \"start\" ] = start data . loc [:, \"end\" ] = end data . loc [:, \"chr\" ] = chr_ # Reorder the columns appropriately data = data . reindex ([ \"chr\" , \"start\" , \"end\" , \"intensity\" ], axis = 1 ) data . to_csv ( fname , sep = \" \\t \" , header = None , index = False ) def expand_seeds ( data : pd . DataFrame , cluster_thresh = 15_000 ) -> pd . DataFrame : DOCS \"\"\"Generate a bedgraph-like DF from the given list of seeds. Each seed is marked in the incoming data by start and end coordinates in its index. The function detect seed clusters, i.e. areas where at least two seeds exist less than 'cluster_thresh' BP apart, and then iterates over all of these clusters and concatenate them together to a new BedGraphFile like structure.\"\"\" shoulders = 5000 starts = data . index . left ends = data . index . right diffs = starts [ 1 :] . to_numpy () - ends [: - 1 ] . to_numpy () diffs = np . concatenate ([[ 0 ], diffs ]) clusters = diffs < cluster_thresh cluster_end_idx = np . where ( ~ clusters )[ 0 ] cluster_start = 0 cluster_data = pd . DataFrame ( np . zeros (( len ( cluster_end_idx ), 2 ), dtype = np . int64 ), columns = [ \"start\" , \"end\" ] ) for row , end in enumerate ( cluster_end_idx ): cluster = data . iloc [ cluster_start : end ] cluster_start = end locus_start = cluster . index [ 0 ] . left - shoulders locus_end = cluster . index [ - 1 ] . right + shoulders cluster_data . iloc [ row , :] = locus_start , locus_end return cluster_data def concat_clusters ( DOCS eco : pd . DataFrame , atac : pd . DataFrame , clusters : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Extracts the data from the 'eco' and 'atac' DFs using the indices in 'clusters'. This function iterates over each cluster as defined in the 'clusters' input and adds the corresponding values from the Chromatin and ATAC datasets into new DataFrames that only contain these clusters. \"\"\" columns = [ \"intensity\" ] clustered_eco = pd . DataFrame ( columns = columns ) clustered_atac = pd . DataFrame ( columns = columns ) for _ , clust in clusters . iterrows (): current_eco = eco . loc [ clust . start : clust . end ] current_atac = atac . loc [ clust . start : clust . end ] clustered_eco = clustered_eco . append ( current_eco ) clustered_atac = clustered_atac . append ( current_atac ) return clustered_eco , clustered_atac def prepare_site_density_for_norm ( theo : pd . DataFrame , quantile : float = 0.1 ) -> Tuple [ pd . Series , pd . DataFrame ]: DOCS \"\"\"Generate a correct theoretical data mask for normalization. The theoretical data is first filtered so that the low quality data is taken out. Then the rest of the data is normalized so that its mean is one. This is sent, as a mask and as the normalization factor, back to the caller so that it could be arbitrarily applied on other data. Parameters ---------- theo: pd.DataFrame Site density values quantile : float, optional The quantile that thresholds the data quality. Data below this quantile will be filtered out Returns ------- blacklist_mask : pd.Series Boolean mask for the relevant parts in the chromosome norm_by : pd.DataFrame Values to multiply with the data for it to be normalized \"\"\" quant = theo . intensity . quantile ( quantile ) blacklist_mask = theo . intensity . isna () | theo . intensity <= quant theo = theo . loc [ ~ blacklist_mask , :] theo . loc [:, \"intensity\" ] /= theo . loc [:, 'intensity' ] . mean () norm_by = 1 / ( theo . loc [:, \"intensity\" ]) return blacklist_mask , norm_by def normalize_with_site_density ( DOCS chrom : pd . DataFrame , naked : pd . DataFrame , theo : pd . DataFrame ) -> Tuple [ pd . DataFrame , pd . DataFrame , pd . DataFrame ]: \"\"\" Use the site density as supplied in the theoretical DF to normalize the values in the other two DFs. First we generate the mask that will be used to discard unneeded events. The mean of the rest of the data is then normalized to one, since the units are arbitrary and it's easier to work with these values (the theo values are relatively low). Then we can apply the same mask to the rest of the data and normalize it by their corresponding theoretical value. Parameters ---------- chrom, naked : pd.DataFrame The data to be normalized theo : pd.DataFrame The data we normalize with Returns ------- (pd.DataFrame, pd.DataFrame, pd.DataFrame) The normalized DFs only at the relevant locations after normalization \"\"\" blacklist_mask , norm_by = prepare_site_density_for_norm ( theo . copy ()) chrom = chrom . loc [ ~ blacklist_mask , :] naked = naked . loc [ ~ blacklist_mask , :] chrom . loc [:, \"intensity\" ] = ( chrom . loc [:, \"intensity\" ] * norm_by ) naked . loc [:, \"intensity\" ] = ( naked . loc [:, \"intensity\" ] * norm_by ) return chrom , naked , theo def preprocess_bedgraph ( paths : List [ pathlib . Path ]) -> List [ BedGraphFile ]: DOCS \"\"\"Run basic pre-processing for the filenames. The function generates a BedGraphFile object and sorts its data attribute. Parameters ---------- paths : List[pathlib.Path] Paths of BedGraphFile files to read Returns ------- List[BedGraphFile] \"\"\" warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) res = [] for path in paths : bed = BedGraphFile ( path , header = False ) bed . data = bed . data . sort_values ( \"start_locus\" ) res . append ( copy . deepcopy ( bed )) return res def subtract_background_with_theo ( DOCS data : pd . DataFrame , no_sites : np . ndarray , ) -> pd . DataFrame : \"\"\"Remove background component and zero-information bins from the data. The bins containing a theoretical value of zero are basically garbage, so we can safely discard them after using them to calculate the noise levels. \"\"\" warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) zero_distrib = data . loc [ no_sites ] baseline_intensity = zero_distrib . mean () data = data . dropna () . clip ( lower = baseline_intensity ) - baseline_intensity data = data . loc [ ~ no_sites ] return data def generate_df_for_theo_correlation_comparison ( DOCS data : pd . DataFrame , theo : pd . DataFrame , nquants : int = 5 , ) -> pd . DataFrame : \"\"\"Creates a long form dataframe that can be used to show correlation between the given theoretical data and some other arbitraty DF. The goal here is to see how do the different values of the `data` variable, be it the naked data or the chromatin one, correlate with the different levels of the theoretical data. Parameters ---------- data : pd.DataFrame The DF from a BedGraphFile object theo : pd.DataFrame Data to compare to nquants : int Number of quantiles to display Returns ------- pd.DataFrame Long-form DF that can be displayed as a Ridge plot \"\"\" data . loc [:, \"quant\" ] = pd . qcut ( theo . intensity , nquants , ) return data def show_ridge_plot ( df : pd . DataFrame , name = \"naked\" ) -> sns . FacetGrid : DOCS \"\"\"Shows the distribution of the data for different categories. Using the output from `generate_df_for_theo_correlation_comparison` we group the data per quantile and display it in several different distribution plots. Taken almost directly from the seaborn gallery. Parameters ---------- df : pd.DataFrame Usually from `generate_df_for_theo_correlation_comparison` name : str, optional Name of the dataset, i.e. 'naked' or 'chromatin' Returns ------- sns.FacetGrid \"\"\" sns . set_theme ( style = \"white\" , rc = { \"axes.facecolor\" : ( 0 , 0 , 0 , 0 )}) # Initialize the FacetGrid object pal = sns . cubehelix_palette ( 10 , rot =- 0.25 , light = 0.7 ) g = sns . FacetGrid ( df , row = \"quant\" , hue = \"quant\" , aspect = 15 , height = 0.8 , palette = pal ) # Draw the densities in a few steps g . map ( sns . kdeplot , \"intensity\" , bw_adjust = 0.5 , clip_on = False , fill = True , alpha = 1 , linewidth = 1.5 , ) g . map ( sns . kdeplot , \"intensity\" , clip_on = False , color = \"w\" , lw = 2 , bw_adjust = 0.5 ) g . map ( plt . axhline , y = 0 , lw = 2 , clip_on = False ) # Define and use a simple function to label the plot in axes coordinates def label ( x , color , label ): ax = plt . gca () ax . text ( 0 , 0.2 , label , fontweight = \"bold\" , color = color , ha = \"left\" , va = \"center\" , transform = ax . transAxes , ) g . map ( label , \"intensity\" ) # Set the subplots to overlap g . fig . subplots_adjust ( hspace =- 0.25 ) # Remove axes details that don't play well with overlap g . set_titles ( \"\" ) g . set ( yticks = []) g . despine ( bottom = True , left = True ) g . fig . suptitle ( f \"figures/ { name . title () } intensity distribution for different quantiles of theoretical values\" ) g . fig . savefig ( f \"figures/ { name . lower () } _intensity_vs_theo_quantiles.png\" , transparent = True , dpi = 300 , ) return g def preprocess_theo ( fname : pathlib . Path ): warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) bed = BedGraphFile ( fname , header = False ) bed . data = bed . data . query ( 'chr == \"chr15\"' ) bed . data = bed . data . sort_values ( \"start_locus\" ) bed = convert_to_intervalindex ([ bed ])[ 0 ] return bed def reindex_theo_data ( naked : pd . DataFrame , theo : pd . DataFrame ) -> pd . DataFrame : warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) new_theo = pd . DataFrame ( { \"chr\" : \"chr15\" , \"intensity\" : np . zeros ( len ( naked ), dtype = np . float64 )} ) for idx in range ( len ( new_theo )): new_theo . iloc [ idx , - 1 ] = theo . loc [ naked . index [ idx ] . left : naked . index [ idx ] . right ] . intensity . mean () newidx = pd . IntervalIndex . from_arrays ( naked . index . left , naked . index . right , closed = \"left\" , name = \"locus\" ) new_theo . loc [:, \"locus\" ] = newidx new_theo = new_theo . set_index ( \"locus\" ) return new_theo def serialize_bedgraph ( bed : BedGraphFile , path : pathlib . Path , chr_ : str = \"chr15\" , mode : str = \"w\" ): warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) data = bed . data data . loc [:, \"left\" ] = data . index . left data . loc [:, \"right\" ] = data . index . right data . loc [:, \"chr\" ] = chr_ data = data . reindex ([ \"chr\" , \"left\" , \"right\" , \"intensity\" ], axis = 1 ) data . to_csv ( path , sep = \" \\t \" , header = None , index = False , mode = mode , ) def reindex_data_with_known_intervals ( intervals , atac , naked , theo , new_index ): newatac = pd . DataFrame ( np . full ( len ( intervals ), np . nan ), index = intervals , columns = [ \"intensity\" ] ) newnaked = pd . DataFrame ( np . full ( len ( intervals ), np . nan ), index = intervals , columns = [ \"intensity\" ] ) newtheo = pd . DataFrame ( np . full ( len ( intervals ), np . nan ), index = intervals , columns = [ \"intensity\" ] ) for int_ in intervals : overlapping_a = atac . data . index . overlaps ( int_ ) overlapping_n = naked . data . index . overlaps ( int_ ) overlapping_t = theo . data . index . overlaps ( int_ ) newatac . loc [ int_ , \"intensity\" ] = atac . data . loc [ overlapping_a , \"intensity\" ] . mean () newnaked . loc [ int_ , \"intensity\" ] = naked . data . loc [ overlapping_n , \"intensity\" ] . mean () newtheo . loc [ int_ , \"intensity\" ] = theo . data . loc [ overlapping_t , \"intensity\" ] . mean () newatac = newatac . reindex ( new_index ) newnaked = newnaked . reindex ( new_index ) newtheo = newtheo . reindex ( new_index ) return newatac , newnaked , newtheo def get_peak_indices ( peaks : pd . DataFrame , data : pd . DataFrame ) -> np . ndarray : DOCS \"\"\"Iterate over the peaks and extract the data index at these points.\"\"\" int_peaks = pd . IntervalIndex . from_arrays ( peaks . start_locus , peaks . end_locus , closed = \"left\" ) res = [] for interval in int_peaks : peak_index = np . where ( data . index . overlaps ( interval ))[ 0 ] if peak_index . size > 0 : res . append ( peak_index [ 0 ]) return np . asarray ( res ) def separate_top_intensity_values ( chrom : pd . DataFrame , naked : pd . DataFrame , peaks : np . ndarray , no_peaks : np . ndarray ): top_eco = chrom . iloc [ peaks ] . loc [:, \"intensity\" ] non_top_eco = chrom . iloc [ no_peaks ] . loc [:, \"intensity\" ] top_naked = naked . iloc [ peaks ] . loc [:, \"intensity\" ] non_top_naked = naked . iloc [ no_peaks ] . loc [:, \"intensity\" ] return top_eco , top_naked , non_top_eco , non_top_naked def scatter_peaks_no_peaks ( top_eco : pd . DataFrame , top_naked : pd . DataFrame , non_top_eco : pd . DataFrame , non_top_naked : pd . DataFrame , ax : plt . Axes = None , ): if not ax : _ , ax = plt . subplots ( figsize = ( 12 , 12 )) ax . set_xlabel ( \"Chromatin\" ) ax . set_ylabel ( \"Naked\" ) ax . scatter ( non_top_eco , non_top_naked , alpha = 0.2 , label = \"All Points\" , ) ax . scatter ( top_eco , top_naked , label = \"Open ATAC\" ) ax . axvline ( non_top_eco . mean (), color = \"C0\" ) ax . axvline ( top_eco . mean (), color = \"C1\" ) ax . axhline ( non_top_naked . mean (), color = \"C0\" ) ax . axhline ( top_naked . mean (), color = \"C1\" ) ax . legend ( loc = \"upper right\" , frameon = False , shadow = False , ) # We concatenate the two DFs to a single one so that the dropna() call will # \"synced\" between the two different rows top = pd . DataFrame ({ \"chrom\" : top_eco , \"naked\" : top_naked }) . dropna ( axis = 0 ) all_ = pd . DataFrame ({ \"chrom\" : non_top_eco , \"naked\" : non_top_naked }) . dropna ( axis = 0 ) r_top , _ = scipy . stats . pearsonr ( top . loc [:, \"chrom\" ], top . loc [:, \"naked\" ]) r_all , _ = scipy . stats . pearsonr ( all_ . loc [:, \"chrom\" ], all_ . loc [:, \"naked\" ]) ax . text ( 0.01 , 0.8 , f \"R (top) = { r_top } \\n R (rest) = { r_all } \" , transform = ax . transAxes ) return ax def classify_open_closed_loci_with_quant ( df : pd . DataFrame , quant : float = 0.1 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: abs_sub = df . abs () q = abs_sub . intensity . quantile ( quant ) open_areas = abs_sub . query ( \"intensity <= @q\" ) closed_areas = abs_sub . query ( \"intensity > @q\" ) return open_areas , closed_areas def normalize_group_peaks_single_factor ( DOCS peaks : np . ndarray , data : pd . DataFrame , norm_to : float = None ): \"\"\"Multiplies the given data by some norm factor, or finds that norm factor. We wish to normalize the two groups, naked and chrom, using the peak data. This function finds the median value of the peaks and uses that value as the go-to target for the other target. \"\"\" peak_median = data . iloc [ peaks ] . median () if not norm_to : return data , peak_median normed = data * ( norm_to / peak_median ) return normed , - 1 def normalize_with_theo ( data : pd . Series , theo : pd . DataFrame ) -> pd . DataFrame : warnings . warn ( 'Deprecated. Please use the BedGraphAccessor-provided methods.' , DeprecationWarning ) norm_by = theo . intensity . dropna () norm_by = 1 / norm_by . loc [ norm_by != 0 ] return data * norm_by def iter_over_bedgraphs_chromosomes ( * args ) -> Iterable [ List [ pd . DataFrame ]]: DOCS \"\"\"Constructs an chromosome iterator for each DF in the given args. The function can take one or more DFs with the chromosome column and bind them together for a generator that only yields the relevant rows of each DF every time. Parameters ---------- One or more DFs with the 'chr' column Returns ------- An iterator that yields a tuple of the DFs only at a certain chromosome \"\"\" grouped = [ df . groupby ( 'chr' , as_index = False ) for df in args ] if len ( grouped ) == 1 : return grouped for chr_ , grp in grouped [ 0 ]: try : others = [ other . get_group ( chr_ ) for other in grouped [ 1 :]] except KeyError : continue yield [ chr_ , grp ] + others","title":"ecodam_py.eco_atac_normalization"},{"location":"api/source/ecodam_py/","text":"SOURCE CODE ecodam_py DOCS __version__ = \"0.1.0\" __title__ = \"EcoDAM-Py\" __description__ = \"Compare and visualize EcoDAM performance\" __url__ = 'https://github.com/ebensteinLab/EcoDAM/' __doc__ = __description__ + \" <\" + __url__ + \">\" __author__ = 'Hagai Har-Gil' __email__ = 'hagaihargil@gmail.com' __license__ = \"MIT\"","title":"ecodam_py"},{"location":"api/source/ecodam_py.ml_pipeline/","text":"SOURCE CODE ecodam_py. ml_pipeline DOCS from typing import Iterable from collections import namedtuple import pandas as pd from ecodam_py.bedgraph import BedGraphAccessor , equalize_loci from ecodam_py.eco_atac_normalization import normalize_with_site_density , serialize_bedgraph , prepare_site_density_for_norm EcoDamData = namedtuple ( \"EcoDamData\" , [ \"chrom\" , \"naked\" , \"theo\" , \"nfr\" ]) def serialize_state ( data : EcoDamData , tag : str ): chr_ = data . chrom . chr . iloc [ 0 ] for item , field in zip ( data , EcoDamData . _fields ): item . to_parquet ( f '/mnt/saphyr/Saphyr_Data/DAM_DLE_VHL_DLE/Hagai/ml_pipeline/ { field } _ { chr_ } _ { tag } .pq' ) def ml_pipeline ( chrom , naked , theo , nfr ): args = EcoDamData ( chrom , naked , theo , nfr ) equalized_chrom , equalized_theo = equalize_loci ( chrom . copy (), theo . copy ()) mask , norm = prepare_site_density_for_norm ( equalized_theo . even ) equalized_naked , equalized_theo = equalize_loci ( naked , theo ) equalized_chrom . even . loc [ ~ mask , \"intensity\" ] *= norm equalized_chrom . even . loc [:, \"intensity\" ] -= equalized_chrom . even . loc [:, \"intensity\" ] . min () equalized_chrom . even . loc [ mask , \"intensity\" ] = 0 equalized_chrom . even . loc [:, \"intensity\" ] /= equalized_chrom . even . loc [:, \"intensity\" ] . max () equalized_naked . even . loc [ ~ mask , \"intensity\" ] *= norm equalized_naked . even . loc [:, \"intensity\" ] -= equalized_naked . even . loc [:, \"intensity\" ] . min () equalized_naked . even . loc [ mask , \"intensity\" ] = 0 equalized_naked . even . loc [:, \"intensity\" ] /= equalized_naked . even . loc [:, \"intensity\" ] . max () subtraction = equalized_chrom . even . copy () subtraction . loc [:, 'intensity' ] = equalized_naked . even . loc [:, 'intensity' ] - equalized_chrom . even . loc [:, 'intensity' ] subtraction = subtraction . dropna ( axis = 0 ) subtraction_overlap , nfr_overlap = subtraction . bg . weighted_overlap ( nfr . copy ()) return subtraction_overlap , nfr_overlap # serialize_state(with_same_bounds, 'after_same_bounds') # subtraction = naked.loc[:, 'intensity'] - chrom.loc[:, 'intensity'] # subtraction.to_parquet('/mnt/saphyr/Saphyr_Data/DAM_DLE_VHL_DLE/Hagai/ml_pipeline/subtraction.pq') # subtraction_nfr = get_index_values_for_nfr(with_same_bounds.nfr, subtraction) # subtraction_nfr_indices = subtraction_nfr.index_to_numpy() # subtraction_nonnfr = subtraction.loc[subtraction.index.difference(subtraction_nfr_indices)] # subtraction_nonnfr_indices = subtraction_nonnfr.index.to_numpy() # return subtraction_nfr, subtraction_nonnfr def subtract_min ( data : pd . DataFrame ) -> pd . DataFrame : min_ = data . intensity . min () data . loc [:, 'intensity' ] -= min_ return data","title":"ecodam_py.ml_pipeline"},{"location":"api/source/ecodam_py.parse_gene_coding/","text":"SOURCE CODE ecodam_py. parse_gene_coding DOCS \"\"\" A filteration script written pretty specifically for some GTF file that Gil needed to be parsed and filtered. It takes about an hour or two to run on the Irys server due to the slow iteration step that is performed internally by the gffutils library. It also reverses the start and end loci on the rows with the - strand, as seen in the find_minus_primer function. \"\"\" import pathlib import pandas as pd import gffutils def find_plus_primer ( start , end ) -> tuple : return ( start - 2000 , start + 500 ) def find_minus_primer ( start , end ) -> tuple : return ( end - 500 , end + 2000 ) find_primer_loc = { '+' : find_plus_primer , '-' : find_minus_primer } def filter_by_required_gene ( data , gene_type ) -> pd . DataFrame : newfile = [] for gene in data . features_of_type ( \"gene\" ): if gene . attributes [ \"gene_type\" ] == required_gene_type : start , end = find_primer_loc [ gene . strand ]( gene . start , gene . end ) newfile . append ( ( gene . chrom , start , end , gene . attributes [ \"gene_name\" ][ 0 ], gene . id , gene . strand , ) ) parsed = pd . DataFrame ( newfile , columns = [ 'chr' , 'start' , 'end' , 'name' , 'id' , 'strand' ]) return parsed if __name__ == '__main__' : fname = pathlib . Path ( \"/home/hagaih/Downloads/gencode.v35.annotation.gtf\" ) data = gffutils . create_db ( str ( fname ), ':memory:' , id_spec = { \"gene\" : \"gene_id\" , \"transcript\" : \"transcript_id\" }, merge_strategy = \"create_unique\" , keep_order = True , ) required_gene_type = [ \"protein_coding\" ] df = filter_by_required_gene ( data , required_gene_type ) df . to_csv ( fname . with_name ( 'gencode.v35.annotation_filtered.tsv' ), sep = ' \\t ' , header = None , index = False )","title":"ecodam_py.parse_gene_coding"},{"location":"api/source/ecodam_py.peak_calling/","text":"SOURCE CODE ecodam_py. peak_calling DOCS \"\"\" Functions accompanying the 'peak_calling' notebook. \"\"\" import pathlib import numpy as np import pandas as pd import matplotlib.pyplot as plt from ecodam_py.bedgraph import BedGraphFile def preprocess_data ( fname : pathlib . Path ) -> BedGraphFile : DOCS \"\"\"A series of simple steps that should be done automatically to BedGraphFile data. Parameters ---------- fname : pathlib.Path BedGraphFile filename Returns ------- BedGraphFile \"\"\" bed = BedGraphFile ( fname , header = False ) data = bed . data . sort_values ( \"start_locus\" ) left = bed . data . loc [:, \"start_locus\" ] . copy () right = bed . data . loc [:, \"end_locus\" ] . copy () data = data . drop ([ \"start_locus\" , \"end_locus\" ], axis = 1 ) index = pd . IntervalIndex . from_arrays ( left , right , closed = \"left\" , name = \"locus\" ) data = data . set_index ( index ) . fillna ({ \"chr\" : \"chr15\" , \"intensity\" : 0 }) bed . data = data return bed def _split_arrays_to_start_ends ( data : np . ndarray , peak_indices : np . ndarray , periods = 3 ): \"\"\"Splits data into segments centered around the given peak_indices. For each peak, this function return values from data that was located in [peak_idx - periods, peak_index + (periods + 1)]. The data is returned as one continguous array, i.e. the distant peaks are concatenated together. Parameters ---------- data : np.ndarray Data to segment peak_indices : np.ndarray Center of segments to cut from data periods : int Number of indices to add before and after the index of the peak Returns ------- np.ndarray A concatenated array of the surrounding area around all given peaks \"\"\" left = peak_indices - periods left [ left < 0 ] = 0 right = peak_indices + ( periods + 1 ) right [ right >= len ( right )] = len ( right ) all_ = np . concatenate ([ left , right ]) all_ . sort () return np . concatenate ( np . split ( data , all_ )[ 1 :: 2 ]) def define_peak_surroundings ( DOCS atac : pd . DataFrame , atac_peak_indices : np . ndarray , eco : pd . DataFrame , eco_peak_indices : np . ndarray , ): \"\"\"Aggregator function to prettify notebook's code. The function merely calls a different one for all relevant data that should be displayed. \"\"\" atac_split = _split_arrays_to_start_ends ( atac . intensity . to_numpy (), atac_peak_indices , periods = 3 ) atac_indices = _split_arrays_to_start_ends ( atac . index . mid , atac_peak_indices , periods = 3 ) eco_split = _split_arrays_to_start_ends ( eco . intensity . to_numpy (), eco_peak_indices , periods = 3 ) eco_indices = _split_arrays_to_start_ends ( eco . index . mid , eco_peak_indices , periods = 3 ) return atac_split , atac_indices , eco_split , eco_indices def color_peak_surroundings ( DOCS atac_split : pd . DataFrame , atac_indices : np . ndarray , eco_split : pd . DataFrame , eco_indices : np . ndarray , ): \"\"\"Generate a plot for the peak-centered BedGraphFile data. The resulting plot is a barplot that overlays the Eco and ATAC data on the same coordinates. \"\"\" _ , ax = plt . subplots () ax . bar ( atac_indices , atac_split , color = \"C2\" , alpha = 0.4 , width = 1000 , label = \"ATAC\" ) ax . bar ( eco_indices , eco_split , color = \"C0\" , alpha = 0.4 , width = 1000 , label = \"EcoDAM\" ) ax . legend ( loc = 0 )","title":"ecodam_py.peak_calling"},{"location":"api/source/ecodam_py.pybam/","text":"SOURCE CODE ecodam_py. pybam DOCS ''' Awesome people who have directly contributed to the project: Jon Palmer - Bug finder & advice on project direction Mahmut Uludag - Bug finder Help: print pybam.wat Github: http://github.com/JohnLonginotto/pybam This code was written by John Longinotto, a PhD student of the Pospisilik Lab at the Max Planck Institute of Immunbiology & Epigenetics, Freiburg. My PhD is funded by the Deutsches Epigenom Programm (DEEP), and the Max Planck IMPRS Program. I study Adipose Biology and Circadian Rhythm in mice, although it seems these days I spend most of my time at the computer :-) ''' import os import sys import zlib import time import tempfile import subprocess from array import array from struct import unpack CtoPy = { 'A' : '<c' , 'c' : '<b' , 'C' : '<B' , 's' : '<h' , 'S' : '<H' , 'i' : '<i' , 'I' : '<I' , 'f' : '<f' } py4py = { 'A' : 1 , 'c' : 1 , 'C' : 1 , 's' : 2 , 'S' : 2 , 'i' : 4 , 'I' : 4 , 'f' : 4 } dna_codes = '=ACMGRSVTWYHKDBN' cigar_codes = 'MIDNSHP=X' parse_codes = { 'sam' : ' The current alignment in SAM format.' , 'bam' : ' All the bytes that make up the current alignment (\"read\"), \\n still in binary just as it was in the BAM file. Useful \\n when creating a new BAM file of filtered alignments.' , 'sam_qname' : ' [1st column in SAM] The QNAME (fragment ID) of the alignment.' , 'bam_qname' : ' The original bytes before decoding to sam_qname.' , 'sam_flag' : ' [2nd column in SAM] The FLAG number of the alignment.' , 'bam_flag' : ' The original bytes before decoding to sam_flag.' , 'sam_refID' : ' The chromosome ID (not the same as the name!). \\n Chromosome names are stored in the BAM header (file_chromosomes), \\n so to convert refIDs to chromsome names one needs to do: \\n \"my_bam.file_chromosomes[read.sam_refID]\" (or use sam_rname) \\n But for comparisons, using the refID is much faster that using \\n the actual chromosome name (for example, when reading through a \\n sorted BAM file and looking for where last_refID != this_refID) \\n Note that when negative the alignment is not aligned, and thus one \\n must not perform my_bam.file_chromosomes[read.sam_refID] \\n without checking that the value is positive first.' , 'sam_rname' : ' [3rd column in SAM] The actual chromosome/contig name for the \\n alignment. Will return \"*\" if refID is negative.' , 'bam_refID' : ' The original bytes before decoding to sam_refID.' , 'sam_pos1' : ' [4th column in SAM] The 1-based position of the alignment. Note \\n that in SAM format values less than 1 are converted to \"0\" for \\n \"no data\" and sam_pos1 will also do this.' , 'sam_pos0' : ' The 0-based position of the alignment. Note that in SAM all \\n positions are 1-based, but in BAM they are stored as 0-based. \\n Unlike sam_pos1, negative values are kept as negative values, \\n essentially giving one the decoded value as it was stored.' , 'bam_pos' : ' The original bytes before decoding to sam_pos*.' , 'sam_mapq' : ' [5th column in SAM] The Mapping Quality of the current alignment.' , 'bam_mapq' : ' The original bytes before decoding to sam_mapq.' , 'sam_cigar_string' : ' [6th column in SAM] The CIGAR string, as per the SAM format. \\n Allowed values are \"MIDNSHP=X\".' , 'sam_cigar_list' : ' A list of tuples with 2 values per tuple: \\n the number of bases, and the CIGAR operation applied to those \\n bases. Faster to calculate than sam_cigar_string.' , 'bam_cigar' : ' The original bytes before decoding to sam_cigar_*.' , 'sam_next_refID' : ' The sam_refID of the alignment \\' s mate (if any). Note that as per \\n sam_refID, this value can be negative and is not the actual \\n chromosome name (see sam_pnext1).' , 'sam_rnext' : ' [7th column in SAM] The chromosome name of the alignment \\' s mate. \\n Value is \"*\" if unmapped. Note that in a SAM file this value \\n is \"=\" if it is the same as the sam_rname, however pybam will \\n only do this if the user prints the whole SAM entry with \"sam\".' , 'bam_next_refID' : ' The original bytes before decoding to sam_next_refID.' , 'sam_pnext1' : ' [8th column in SAM] The 1-based position of the alignment \\' s mate. \\n Note that in SAM format values less than 1 are converted to \"0\" \\n for \"no data\", and sam_pnext1 will also do this.' , 'sam_pnext0' : ' The 0-based position of the alignment \\' s mate. Note that in SAM all \\n positions are 1-based, but in BAM they are stored as 0-based. \\n Unlike sam_pnext1, negative values are kept as negative values \\n here, essentially giving you the value as it was stored in BAM.' , 'bam_pnext' : ' The original bytes before decoding to sam_pnext0.' , 'sam_tlen' : ' [9th column in SAM] The TLEN value.' , 'bam_tlen' : ' The original bytes before decoding to sam_tlen.' , 'sam_seq' : ' [10th column in SAM] The SEQ value (DNA sequence of the alignment). \\n Allowed values are \"ACGTMRSVWYHKDBN and =\".' , 'bam_seq' : ' The original bytes before decoding to sam_seq.' , 'sam_qual' : ' [11th column in SAM] The QUAL value (quality scores per DNA base \\n in SEQ) of the alignment.' , 'bam_qual' : ' The original bytes before decoding to sam_qual.' , 'sam_tags_list' : ' A list of tuples with 3 values per tuple: a two-letter TAG ID, the \\n type code used to describe the data in the TAG value (see SAM spec. \\n for details), and the value of the TAG. Note that the BAM format \\n has type codes like \"c\" for a number in the range -127 to +127, \\n and \"C\" for a number in the range of 0 to 255. \\n In a SAM file however, all numerical codes appear to just be stored \\n using \"i\", which is a number in the range -2147483647 to +2147483647. \\n sam_tags_list will therefore return the code used in the BAM file, \\n and not \"i\" for all numbers.' , 'sam_tags_string' : ' [12th column a SAM] Returns the TAGs in the same format as would be found \\n in a SAM file (with all numbers having a signed 32bit code of \"i\").' , 'bam_tags' : ' The original bytes before decoding to sam_tags_*.' , 'sam_bin' : ' The bin value of the alignment (used for indexing reads). \\n Please refer to section 5.3 of the SAM spec for how this \\n value is calculated.' , 'bam_bin' : ' The original bytes before decoding to sam_bin.' , 'sam_block_size' : ' The number of bytes the current alignment takes up in the BAM \\n file minus the four bytes used to store the block_size value \\n itself. Essentially sam_block_size +4 == bytes needed to store \\n the current alignment.' , 'bam_block_size' : ' The original bytes before decoding to sam_block_size.' , 'sam_l_read_name' : ' The length of the QNAME plus 1 because the QNAME is terminated \\n with a NUL byte.' , 'bam_l_read_name' : ' The original bytes before decoding to sam_l_read_name.' , 'sam_l_seq' : ' The number of bases in the seq. Useful if you just want to know \\n how many bases are in the SEQ but do not need to know what those \\n bases are (which requires more decoding effort).' , 'bam_l_seq' : ' The original bytes before decoding to sam_l_seq.' , 'sam_n_cigar_op' : ' The number of CIGAR operations in the CIGAR field. Useful if one \\n wants to know how many CIGAR operations there are, but does not \\n need to know what they are.' , 'bam_n_cigar_op' : ' The original bytes before decoding to sam_n_cigar_op.' , 'file_alignments_read' : ' A running counter of the number of alignments (\"reads\"), \\n processed thus far. Note the BAM format does not store \\n how many reads are in a file, so the usefulness of this \\n metric is somewhat limited unless one already knows how \\n many reads are in the file.' , 'file_binary_header' : ' From the first byte in the file, until the first byte of \\n the first read. The original binary header.' , 'file_bytes_read' : ' A running counter of the bytes read from the file. Note \\n that as data is read in arbitary chunks, this is literally \\n the amount of data read from the file/pipe by pybam.' , 'file_chromosome_lengths' : ' The binary header of the BAM file includes chromosome names \\n and chromosome lengths. This is a dictionary of chromosome-name \\n keys and chromosome-length values.' , 'file_chromosomes' : ' A list of chromosomes from the binary header.' , 'file_decompressor' : ' BAM files are compressed with bgzip. The value here reflects \\n the decompressor used. \"internal\" if pybam \\' s internal \\n decompressor is being used, \"gzip\" or \"pigz\" if the system \\n has these binaries installed and pybam can find them. \\n Any other value reflects a custom decompression command.' , 'file_directory' : ' The directory the input BAM file can be found in. This will be \\n correct if the input file is specified via a string or python \\n file object, however if the input is a pipe such as sys.stdin, \\n then the current working directory will be used.' , 'file_header' : ' The ASCII portion of the BAM header. This is the typical header \\n users of samtools will be familiar with.' , 'file_name' : ' The file name (base name) of input file if input is a string or \\n python file object. If input is via stdin this will be \"<stdin>\"' } wat = ''' Main class: pybam.read Github: http://github.com/JohnLonginotto/pybam [ Dynamic Parser Example ] for alignment in pybam.read('/my/data.bam'): print alignment.sam_seq [ Static Parser Example ] for seq,mapq in pybam.read('/my/data.bam',['sam_seq','sam_mapq']): print seq print mapq [ Mixed Parser Example ] my_bam = pybam.read('/my/data.bam',['sam_seq','sam_mapq']) print my_bam._static_parser_code for seq,mapq in my_bam: if seq.startswith('ACGT') and mapq > 10: print my_bam.sam [ Custom Decompressor (from file path) Example ] my_bam = pybam.read('/my/data.bam.lzma',decompressor='lzma --decompress --stdout /my/data.bam.lzma') [ Custom Decompressor (from file object) Example ] my_bam = pybam.read(sys.stdin,decompressor='lzma --decompress --stdout') # data given to lzma via stdin [ Force Internal bgzip Decompressor ] my_bam = pybam.read('/my/data.bam',decompressor='internal') [ Parse Words (hah) ]''' wat += ' \\n ' + '' . join ([( ' \\n =============================================================================================== \\n\\n ' if code is 'file_alignments_read' or code is 'sam' else ' ' ) + ( code + ' ' ) . ljust ( 25 , '-' ) + description + ' \\n ' for code , description in sorted ( parse_codes . items ())]) + ' \\n ' class read : DOCS ''' [ Dynamic Parser Example ] for alignment in pybam.read('/my/data.bam'): print alignment.sam_seq [ Static Parser Example ] for seq,mapq in pybam.read('/my/data.bam',['sam_seq','sam_mapq']): print seq print mapq [ Mixed Parser Example ] my_bam = pybam.read('/my/data.bam',['sam_seq','sam_mapq']) print my_bam._static_parser_code for seq,mapq in my_bam: if seq.startswith('ACGT') and mapq > 10: print my_bam.sam [ Custom Decompressor (from file path) Example ] my_bam = pybam.read('/my/data.bam.lzma',decompressor='lzma --decompress --stdout /my/data.bam.lzma') [ Custom Decompressor (from file object) Example ] my_bam = pybam.read(sys.stdin,decompressor='lzma --decompress --stdout') # data given to lzma via stdin [ Force Internal bgzip Decompressor ] my_bam = pybam.read('/my/data.bam',decompressor='internal') \"print pybam.wat\" in the python terminal to see the possible parsable values, or visit http://github.com/JohnLonginotto/pybam for the latest info. ''' def __init__ ( self , f , fields = False , decompressor = False ): self . file_bytes_read = 0 self . file_chromosomes = [] self . file_alignments_read = 0 self . file_chromosome_lengths = {} if fields is not False : if type ( fields ) is not list or len ( fields ) is 0 : raise PybamError ( ' \\n\\n Fields for the static parser must be provided as a non-empty list. You gave a ' + str ( type ( fields )) + ' \\n ' ) else : for field in fields : if field . startswith ( 'sam' ) or field . startswith ( 'bam' ): if field not in parse_codes . keys (): raise PybamError ( ' \\n\\n Static parser field \"' + str ( field ) + '\" from fields ' + str ( fields ) + ' is not known to this version of pybam! \\n Print \"pybam.wat\" to see available field names with explinations. \\n ' ) else : raise PybamError ( ' \\n\\n Static parser field \"' + str ( field ) + '\" from fields ' + str ( fields ) + ' does not start with \"sam\" or \"bam\" and thus is not an avaliable field for the static parsing. \\n Print \"pybam.wat\" in interactive python to see available field names with explinations. \\n ' ) if decompressor : if type ( decompressor ) is str : if decompressor != 'internal' and ' {} ' not in decompressor : raise PybamError ( ' \\n\\n When a custom decompressor is used and the input file is a string, the decompressor string must contain at least one occurence of \" {} \" to be substituted with a filepath by pybam. \\n ' ) else : raise PybamError ( ' \\n\\n User-supplied decompressor must be a string that when run on the command line decompresses a named file (or stdin), to stdout: \\n e.g. \"lzma --decompress --stdout {} \" if pybam is provided a path as input file, where {} is substituted for that path. \\n or just \"lzma --decompress --stdout\" if pybam is provided a file object instead of a file path, as data from that file object will be piped via stdin to the decompression program. \\n ' ) ## First we make a generator that will return chunks of uncompressed data, regardless of how we choose to decompress: def generator (): DEVNULL = open ( os . devnull , 'wb' ) # First we need to figure out what sort of file we have - whether it's gzip compressed, uncompressed, or something else entirely! if type ( f ) is str : try : self . _file = open ( f , 'rb' ) except : raise PybamError ( ' \\n\\n Could not open \"' + str ( self . _file . name ) + '\" for reading! \\n ' ) try : magic = os . read ( self . _file . fileno (), 4 ) except : raise PybamError ( ' \\n\\n Could not read from \"' + str ( self . _file . name ) + '\"! \\n ' ) elif type ( f ) is file : self . _file = f try : magic = os . read ( self . _file . fileno (), 4 ) except : raise PybamError ( ' \\n\\n Could not read from \"' + str ( self . _file . name ) + '\"! \\n ' ) else : raise PybamError ( ' \\n\\n Input file was not a string or a file object. It was: \"' + str ( f ) + '\" \\n ' ) self . file_name = os . path . basename ( os . path . realpath ( self . _file . name )) self . file_directory = os . path . dirname ( os . path . realpath ( self . _file . name )) if magic == 'BAM \\1 ' : # The user has passed us already unzipped BAM data! Job done :) data = 'BAM \\1 ' + self . _file . read ( 35536 ) self . file_bytes_read += len ( data ) self . file_decompressor = 'None' while data : yield data data = self . _file . read ( 35536 ) self . file_bytes_read += len ( data ) self . _file . close () DEVNULL . close () raise StopIteration elif magic == \" \\x1f\\x8b\\x08\\x04 \" : # The user has passed us compressed gzip/bgzip data, which is typical for a BAM file # use custom decompressor if provided: if decompressor is not False and decompressor != 'internal' : if type ( f ) is str : self . _subprocess = subprocess . Popen ( decompressor . replace ( ' {} ' , f ), shell = True , stdout = subprocess . PIPE , stderr = DEVNULL ) else : self . _subprocess = subprocess . Popen ( '{ printf \"' + magic + '\"; cat; } | ' + decompressor , stdin = self . _file , shell = True , stdout = subprocess . PIPE , stderr = DEVNULL ) self . file_decompressor = decompressor data = self . _subprocess . stdout . read ( 35536 ) self . file_bytes_read += len ( data ) while data : yield data data = self . _subprocess . stdout . read ( 35536 ) self . file_bytes_read += len ( data ) self . _file . close () DEVNULL . close () raise StopIteration # else look for pigz or gzip: else : try : self . _subprocess = subprocess . Popen ([ \"pigz\" ], stdin = DEVNULL , stdout = DEVNULL , stderr = DEVNULL ) if self . _subprocess . returncode is None : self . _subprocess . kill () use = 'pigz' except OSError : try : self . _subprocess = subprocess . Popen ([ \"gzip\" ], stdin = DEVNULL , stdout = DEVNULL , stderr = DEVNULL ) if self . _subprocess . returncode is None : self . _subprocess . kill () use = 'gzip' except OSError : use = 'internal' if use != 'internal' and decompressor != 'internal' : if type ( f ) is str : self . _subprocess = subprocess . Popen ([ use , '--decompress' , '--stdout' , f ], stdout = subprocess . PIPE , stderr = DEVNULL ) else : self . _subprocess = subprocess . Popen ( '{ printf \"' + magic + '\"; cat; } | ' + use + ' --decompress --stdout' , stdin = f , shell = True , stdout = subprocess . PIPE , stderr = DEVNULL ) time . sleep ( 1 ) if self . _subprocess . poll () == None : data = self . _subprocess . stdout . read ( 35536 ) self . file_decompressor = use self . file_bytes_read += len ( data ) while data : yield data data = self . _subprocess . stdout . read ( 35536 ) self . file_bytes_read += len ( data ) self . _file . close () DEVNULL . close () raise StopIteration # Python's gzip module can't read from a stream that doesn't support seek(), and the zlib module cannot read the bgzip format without a lot of help: self . file_decompressor = 'internal' raw_data = magic + self . _file . read ( 65536 ) self . file_bytes_read = len ( raw_data ) internal_cache = [] blocks_left_to_grab = 50 bs = 0 checkpoint = 0 decompress = zlib . decompress while raw_data : if len ( raw_data ) - bs < 35536 : raw_data = raw_data [ bs :] + self . _file . read ( 65536 ) self . file_bytes_read += len ( raw_data ) - bs bs = 0 magic = raw_data [ bs : bs + 4 ] if not magic : break # a child's heart if magic != \" \\x1f\\x8b\\x08\\x04 \" : raise PybamError ( ' \\n\\n The input file is not in a format I understand. First four bytes: ' + repr ( magic ) + ' \\n ' ) try : more_bs = bs + unpack ( \"<H\" , raw_data [ bs + 16 : bs + 18 ])[ 0 ] + 1 internal_cache . append ( decompress ( raw_data [ bs + 18 : more_bs - 8 ], - 15 )) bs = more_bs except : ## zlib doesnt have a nice exception for when things go wrong. just \"error\" header_data = magic + raw_data [ bs + 4 : bs + 12 ] header_size = 12 extra_len = unpack ( \"<H\" , header_data [ - 2 :])[ 0 ] while header_size - 12 < extra_len : header_data += raw_data [ bs + 12 : bs + 16 ] subfield_id = header_data [ - 4 : - 2 ] subfield_len = unpack ( \"<H\" , header_data [ - 2 :])[ 0 ] subfield_data = raw_data [ bs + 16 : bs + 16 + subfield_len ] header_data += subfield_data header_size += subfield_len + 4 if subfield_id == 'BC' : block_size = unpack ( \"<H\" , subfield_data )[ 0 ] raw_data = raw_data [ bs + 16 + subfield_len : bs + 16 + subfield_len + block_size - extra_len - 19 ] crc_data = raw_data [ bs + 16 + subfield_len + block_size - extra_len - 19 : bs + 16 + subfield_len + block_size - extra_len - 19 + 8 ] # I have left the numbers in verbose, because the above try is the optimised code. bs = bs + 16 + subfield_len + block_size - extra_len - 19 + 8 zipped_data = header_data + raw_data + crc_data internal_cache . append ( decompress ( zipped_data , 47 )) # 31 works the same as 47. # Although the following in the bgzip code from biopython, its not needed if you let zlib decompress the whole zipped_data, header and crc, because it checks anyway (in C land) # I've left the manual crc checks in for documentation purposes: ''' expected_crc = crc_data[:4] expected_size = unpack(\"<I\", crc_data[4:])[0] if len(unzipped_data) != expected_size: print 'ERROR: Failed to unpack due to a Type 1 CRC error. Could the BAM be corrupted?'; exit() crc = zlib.crc32(unzipped_data) if crc < 0: crc = pack(\"<i\", crc) else: crc = pack(\"<I\", crc) if expected_crc != crc: print 'ERROR: Failed to unpack due to a Type 2 CRC error. Could the BAM be corrupted?'; exit() ''' blocks_left_to_grab -= 1 if blocks_left_to_grab == 0 : yield '' . join ( internal_cache ) internal_cache = [] blocks_left_to_grab = 50 self . _file . close () DEVNULL . close () if internal_cache != '' : yield '' . join ( internal_cache ) raise StopIteration elif decompressor is not False and decompressor != 'internal' : # It wouldn't be safe to just print to the shell four random bytes from the beginning of a file, so instead it's # written to a temp file and cat'd. The idea here being that we trust the decompressor string as it was written by # someone with access to python, so it has system access anyway. The file/data, however, should not be trusted. magic_file = os . path . join ( tempfile . mkdtemp (), 'magic' ) with open ( magic_file , 'wb' ) as mf : mf . write ( magic ) if type ( f ) is str : self . _subprocess = subprocess . Popen ( decompressor . replace ( ' {} ' , f ), shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) else : self . _subprocess = subprocess . Popen ( '{ cat \"' + magic_file + '\"; cat; } | ' + decompressor , stdin = self . _file , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) self . file_decompressor = decompressor data = self . _subprocess . stdout . read ( 35536 ) self . file_bytes_read += len ( data ) while data : yield data data = self . _subprocess . stdout . read ( 35536 ) self . file_bytes_read += len ( data ) self . _file . close () DEVNULL . close () raise StopIteration else : raise PybamError ( ' \\n\\n The input file is not in a format I understand. First four bytes: ' + repr ( magic ) + ' \\n ' ) ## At this point, we know that whatever decompression method was used, a call to self._generator will return some uncompressed data. self . _generator = generator () ## So lets parse the BAM header: header_cache = '' while len ( header_cache ) < 8 : header_cache += next ( self . _generator ) p_from = 0 ; p_to = 4 if header_cache [ p_from : p_to ] != 'BAM \\1 ' : raise PybamError ( ' \\n\\n Input file ' + self . file_name + ' does not appear to be a BAM file. \\n ' ) ## Parse the BAM header: p_from = p_to ; p_to += 4 length_of_header = unpack ( '<i' , header_cache [ p_from : p_to ])[ 0 ] p_from = p_to ; p_to += length_of_header while len ( header_cache ) < p_to : header_cache += next ( self . _generator ) self . file_header = header_cache [ p_from : p_to ] p_from = p_to ; p_to += 4 while len ( header_cache ) < p_to : header_cache += next ( self . _generator ) number_of_reference_sequences = unpack ( '<i' , header_cache [ p_from : p_to ])[ 0 ] for _ in range ( number_of_reference_sequences ): p_from = p_to ; p_to += 4 while len ( header_cache ) < p_to : header_cache += next ( self . _generator ) l_name = unpack ( '<l' , header_cache [ p_from : p_to ])[ 0 ] p_from = p_to ; p_to += l_name while len ( header_cache ) < p_to : header_cache += next ( self . _generator ) self . file_chromosomes . append ( header_cache [ p_from : p_to - 1 ]) p_from = p_to ; p_to += 4 while len ( header_cache ) < p_to : header_cache += next ( self . _generator ) self . file_chromosome_lengths [ self . file_chromosomes [ - 1 ]] = unpack ( '<l' , header_cache [ p_from : p_to ])[ 0 ] self . file_bytes_read = p_to self . file_binary_header = buffer ( header_cache [: p_to ]) header_cache = header_cache [ p_to :] # A quick check to make sure the header of this BAM file makes sense: chromosomes_from_header = [] for line in self . file_header . split ( ' \\n ' ): if line . startswith ( '@SQ \\t SN:' ): chromosomes_from_header . append ( line . split ( ' \\t ' )[ 1 ][ 3 :]) if chromosomes_from_header != self . file_chromosomes : raise PybamWarn ( 'For some reason the BAM format stores the chromosome names in two locations, \\n the ASCII text header we all know and love, viewable with samtools view -H, and another special binary header \\n which is used to translate the chromosome refID (a number) into a chromosome RNAME when you do bam -> sam. \\n\\n These two headers should always be the same, but apparently they are not: \\n The ASCII header looks like: ' + self . file_header + ' \\n While the binary header has the following chromosomes: ' + self . file_chromosomes + ' \\n ' ) ## Variable parsing: def new_entry ( header_cache ): cache = header_cache # we keep a small cache of X bytes of decompressed BAM data, to smoothen out disk access. p = 0 # where the next alignment/entry starts in the cache while True : try : while len ( cache ) < p + 4 : cache = cache [ p :] + next ( self . _generator ); p = 0 # Grab enough bytes to parse blocksize self . sam_block_size = unpack ( '<i' , cache [ p : p + 4 ])[ 0 ] self . file_alignments_read += 1 while len ( cache ) < p + 4 + self . sam_block_size : cache = cache [ p :] + next ( self . _generator ); p = 0 # Grab enough bytes to parse entry except StopIteration : break self . bam = cache [ p : p + 4 + self . sam_block_size ] p = p + 4 + self . sam_block_size yield self self . _new_entry = new_entry ( header_cache ) def compile_parser ( self , fields ): temp_code = '' end_of_qname = False end_of_cigar = False end_of_seq = False end_of_qual = False dependencies = set ( fields ) if 'bam' in fields : fields [ fields . index ( 'bam' )] = 'self.bam' if 'sam_block_size' in fields : fields [ fields . index ( 'sam_block_size' )] = 'self.sam_block_size' if 'sam' in dependencies : dependencies . update ([ 'sam_qname' , 'sam_flag' , 'sam_rname' , 'sam_pos1' , 'sam_mapq' , 'sam_cigar_string' , 'bam_refID' , 'bam_next_refID' , 'sam_rnext' , 'sam_pnext1' , 'sam_tlen' , 'sam_seq' , 'sam_qual' , 'sam_tags_string' ]) if 'sam_tags_string' in dependencies : dependencies . update ([ 'sam_tags_list' ]) if 'sam_pos1' in dependencies : temp_code += \" \\n sam_pos1 = (0 if sam_pos0 < 0 else sam_pos0 + 1)\" dependencies . update ([ 'sam_pos0' ]) if 'sam_pnext1' in dependencies : temp_code += \" \\n sam_pnext1 = (0 if sam_pnext0 < 0 else sam_pnext0 + 1)\" dependencies . update ([ 'sam_pnext0' ]) if 'sam_qname' in dependencies or 'bam_qname' in dependencies : temp_code += \" \\n _end_of_qname = 36 + sam_l_read_name\" dependencies . update ([ 'sam_l_read_name' ]) end_of_qname = True if 'sam_cigar_string' in dependencies or 'sam_cigar_list' in dependencies or 'bam_cigar' in dependencies : if end_of_qname : pass else : temp_code += \" \\n _end_of_qname = 36 + sam_l_read_name\" temp_code += \" \\n _end_of_cigar = _end_of_qname + (4*sam_n_cigar_op)\" dependencies . update ([ 'sam_l_read_name' , 'sam_n_cigar_op' ]) end_of_cigar = True if 'sam_seq' in dependencies or 'bam_seq' in dependencies : if end_of_cigar : pass elif end_of_qname : temp_code += \" \\n _end_of_cigar = _end_of_qname + (4*sam_n_cigar_op)\" else : temp_code += \" \\n _end_of_cigar = 36 + sam_l_read_name + (4*sam_n_cigar_op)\" temp_code += \" \\n _end_of_seq = _end_of_cigar + (-((-sam_l_seq)//2))\" dependencies . update ([ 'sam_l_seq' , 'sam_n_cigar_op' , 'sam_l_read_name' ]) end_of_seq = True if 'sam_qual' in dependencies or 'bam_qual' in dependencies : if end_of_seq : pass elif end_of_cigar : temp_code += \" \\n _end_of_seq = _end_of_cigar + (-((-sam_l_seq)//2))\" elif end_of_qname : temp_code += \" \\n _end_of_seq = _end_of_qname + (4*sam_n_cigar_op) + (-((-sam_l_seq)//2))\" else : temp_code += \" \\n _end_of_seq = 36 + sam_l_read_name + (4*sam_n_cigar_op) + (-((-sam_l_seq)//2))\" temp_code += \" \\n _end_of_qual = _end_of_seq + sam_l_seq\" dependencies . update ([ 'sam_l_seq' , 'sam_n_cigar_op' , 'sam_l_read_name' ]) end_of_qual = True if 'sam_tags_list' in dependencies or 'bam_tags' in dependencies : if end_of_qual : pass elif end_of_seq : temp_code += \" \\n _end_of_qual = _end_of_seq + sam_l_seq\" elif end_of_cigar : temp_code += \" \\n _end_of_qual = _end_of_cigar + (-((-sam_l_seq)//2)) + sam_l_seq\" elif end_of_qname : temp_code += \" \\n _end_of_qual = _end_of_qname + (4*sam_n_cigar_op) + (-((-sam_l_seq)//2)) + sam_l_seq\" else : temp_code += \" \\n _end_of_qual = 36 + sam_l_read_name + (4*sam_n_cigar_op) + (-((-sam_l_seq)//2)) + sam_l_seq\" dependencies . update ([ 'sam_l_seq' , 'sam_n_cigar_op' , 'sam_l_read_name' ]) if 'sam_rname' in dependencies : temp_code += \" \\n sam_rname = '*' if sam_refID < 0 else self.file_chromosomes[sam_refID]\" dependencies . update ([ 'sam_refID' ]) if 'sam_rnext' in dependencies : temp_code += \" \\n sam_rnext = '*' if sam_next_refID < 0 else self.file_chromosomes[sam_next_refID]\" dependencies . update ([ 'sam_next_refID' ]) ## First we figure out what data from the static portion of the BAM entry we'll need: tmp = {} tmp [ 'code' ] = 'def parser(self): \\n from array import array \\n from struct import unpack \\n for _ in self._new_entry:' tmp [ 'last_start' ] = None tmp [ 'name_list' ] = [] tmp [ 'dtype_list' ] = [] def pack_up ( name , dtype , length , end , tmp ): if name in dependencies : if tmp [ 'last_start' ] is None : tmp [ 'last_start' ] = end - length tmp [ 'name_list' ] . append ( name ) tmp [ 'dtype_list' ] . append ( dtype ) elif tmp [ 'last_start' ] is not None : tmp [ 'code' ] += ' \\n ' + ', ' . join ( tmp [ 'name_list' ]) + ' = unpack(\"<' + '' . join ( tmp [ 'dtype_list' ]) + '\",self.bam[' + str ( tmp [ 'last_start' ]) + ':' + str ( end - length ) + '])' if len ( tmp [ 'dtype_list' ]) == 1 : tmp [ 'code' ] += '[0]' tmp [ 'last_start' ] = None tmp [ 'name_list' ] = [] tmp [ 'dtype_list' ] = [] pack_up ( 'sam_refID' , 'i' , 4 , 8 , tmp ) pack_up ( 'sam_pos0' , 'i' , 4 , 12 , tmp ) pack_up ( 'sam_l_read_name' , 'B' , 1 , 13 , tmp ) pack_up ( 'sam_mapq' , 'B' , 1 , 14 , tmp ) pack_up ( 'sam_bin' , 'H' , 2 , 16 , tmp ) pack_up ( 'sam_n_cigar_op' , 'H' , 2 , 18 , tmp ) pack_up ( 'sam_flag' , 'H' , 2 , 20 , tmp ) pack_up ( 'sam_l_seq' , 'i' , 4 , 24 , tmp ) pack_up ( 'sam_next_refID' , 'i' , 4 , 28 , tmp ) pack_up ( 'sam_pnext0' , 'i' , 4 , 32 , tmp ) pack_up ( 'sam_tlen' , 'i' , 4 , 36 , tmp ) pack_up ( None , None , 0 , 36 , tmp ) # To add anything not yet added. code = tmp [ 'code' ] del tmp code += temp_code # Fixed-length BAM data (where we just grab the bytes, we dont unpack) can, however, be grabbed individually. if 'bam_block_size' in dependencies : code += \" \\n bam_block_size = self.bam[0 : 4 ]\" if 'bam_refID' in dependencies : code += \" \\n bam_refID = self.bam[4 : 8 ]\" if 'bam_pos' in dependencies : code += \" \\n bam_pos = self.bam[8 : 12 ]\" if 'bam_l_read_name' in dependencies : code += \" \\n bam_l_read_name = self.bam[12 : 13 ]\" if 'bam_mapq' in dependencies : code += \" \\n bam_mapq = self.bam[13 : 14 ]\" if 'bam_bin' in dependencies : code += \" \\n bam_bin = self.bam[14 : 16 ]\" if 'bam_n_cigar_op' in dependencies : code += \" \\n bam_n_cigar_op = self.bam[16 : 18 ]\" if 'bam_flag' in dependencies : code += \" \\n bam_flag = self.bam[18 : 20 ]\" if 'bam_l_seq' in dependencies : code += \" \\n bam_l_seq = self.bam[20 : 24 ]\" if 'bam_next_refID' in dependencies : code += \" \\n bam_next_refID = self.bam[24 : 28 ]\" if 'bam_pnext' in dependencies : code += \" \\n bam_pnext = self.bam[28 : 32 ]\" if 'bam_tlen' in dependencies : code += \" \\n bam_tlen = self.bam[32 : 36 ]\" if 'bam_qname' in dependencies : code += \" \\n bam_qname = self.bam[36 : _end_of_qname ]\" if 'bam_cigar' in dependencies : code += \" \\n bam_cigar = self.bam[_end_of_qname : _end_of_cigar ]\" if 'bam_seq' in dependencies : code += \" \\n bam_seq = self.bam[_end_of_cigar : _end_of_seq ]\" if 'bam_qual' in dependencies : code += \" \\n bam_qual = self.bam[_end_of_seq : _end_of_qual ]\" if 'bam_tags' in dependencies : code += \" \\n bam_tags = self.bam[_end_of_qual : ]\" if 'sam_qname' in dependencies : if 'bam_qname' in dependencies : code += \" \\n sam_qname = bam_qname[:-1]\" else : code += \" \\n sam_qname = self.bam[36 : _end_of_qname -1 ]\" if 'sam_cigar_list' in dependencies : if 'bam_cigar' in dependencies : code += \" \\n sam_cigar_list = [( cig >> 4 , cigar_codes[cig & 0b1111]) for cig in array('I', bam_cigar) ]\" else : code += \" \\n sam_cigar_list = [( cig >> 4 , cigar_codes[cig & 0b1111]) for cig in array('I', self.bam[_end_of_qname : _end_of_cigar]) ]\" if 'sam_cigar_string' in dependencies : if 'bam_cigar' in dependencies : code += \" \\n sam_cigar_string = ''.join([ str(cig >> 4) + cigar_codes[cig & 0b1111] for cig in array('I', bam_cigar)])\" else : code += \" \\n sam_cigar_string = ''.join([ str(cig >> 4) + cigar_codes[cig & 0b1111] for cig in array('I', self.bam[_end_of_qname : _end_of_cigar]) ])\" if 'sam_seq' in dependencies : if 'bam_seq' in dependencies : code += \" \\n sam_seq = ''.join( [ dna_codes[dna >> 4] + dna_codes[dna & 0b1111] for dna in array('B', bam_seq)])[:sam_l_seq]\" else : code += \" \\n sam_seq = ''.join( [ dna_codes[dna >> 4] + dna_codes[dna & 0b1111] for dna in array('B', self.bam[_end_of_cigar : _end_of_seq])])[:sam_l_seq]\" if 'sam_qual' in dependencies : if 'bam_qual' in dependencies : code += \" \\n sam_qual = ''.join( [ chr(ord(quality) + 33) for quality in bam_qual ])\" else : code += \" \\n sam_qual = ''.join( [ chr(ord(quality) + 33) for quality in self.bam[_end_of_seq : _end_of_qual ]])\" if 'sam_tags_list' in dependencies : code += ''' sam_tags_list = [] offset = _end_of_qual while offset != len(self.bam): tag_name = self.bam[offset:offset+2] tag_type = self.bam[offset+2] if tag_type == 'Z': offset_end = self.bam.index(' \\\\ 0',offset+3)+1 tag_data = self.bam[offset+3:offset_end-1] elif tag_type in CtoPy: offset_end = offset+3+py4py[tag_type] tag_data = unpack(CtoPy[tag_type],self.bam[offset+3:offset_end])[0] elif tag_type == 'B': offset_end = offset+8+(unpack('<i',self.bam[offset+4:offset+8])[0]*py4py[self.bam[offset+3]]) tag_data = array(self.bam[offset+3] , self.bam[offset+8:offset_end] ) else: print 'PYBAM ERROR: I dont know how to parse BAM tags in this format: ',repr(tag_type) print ' This is simply because I never saw this kind of tag during development.' print ' If you could mail the following chunk of text to john at john.uk.com, i will fix this up for everyone :)' print repr(tag_type),repr(self.bam[offset+3:end]) exit() sam_tags_list.append((tag_name,tag_type,tag_data)) offset = offset_end''' if 'sam_tags_string' in dependencies : code += \" \\n sam_tags_string = ' \\t '.join(A + ':' + ('i' if B in 'cCsSI' else B) + ':' + ((C.typecode + ',' + ','.join(map(str,C))) if type(C)==array else str(C)) for A,B,C in self.sam_tags_list)\" if 'sam' in dependencies : code += \" \\n sam = sam_qname + ' \\t ' + str(sam_flag) + ' \\t ' + sam_rname + ' \\t ' + str(sam_pos1) + ' \\t ' + str(sam_mapq) + ' \\t ' + ('*' if sam_cigar_string == '' else sam_cigar_string) + ' \\t ' + ('=' if bam_refID == bam_next_refID else sam_rnext) + ' \\t ' + str(sam_pnext1) + ' \\t ' + str(sam_tlen) + ' \\t ' + sam_seq + ' \\t ' + sam_qual + ' \\t ' + sam_tags_string\" code += ' \\n yield ' + ',' . join ([ x for x in fields ]) + ' \\n ' self . _static_parser_code = code # \"code\" is the static parser's code as a string (a function called \"parser\") exec_dict = { # This dictionary stores things the exec'd code needs to know about, and will store the compiled function after exec() 'unpack' : unpack , 'array' : array , 'dna_codes' : dna_codes , 'CtoPy' : CtoPy , 'py4py' : py4py , 'cigar_codes' : cigar_codes } exec ( code in exec_dict ) # exec() compiles \"code\" to real code, creating the \"parser\" function and adding it to exec_dict['parser'] return exec_dict [ 'parser' ] if fields : static_parser = compile_parser ( self , fields )( self ) def next_read (): return next ( static_parser ) else : def next_read (): return next ( self . _new_entry ) self . next = next_read def __iter__ ( self ): return self def __str__ ( self ): return self . sam ## Methods to pull out raw bam data from entry (so still in its binary encoding). This can be helpful in some scenarios. @property def bam_block_size ( self ): return self . bam [ : 4 ] @property def bam_refID ( self ): return self . bam [ 4 : 8 ] @property def bam_pos ( self ): return self . bam [ 8 : 12 ] @property def bam_l_read_name ( self ): return self . bam [ 12 : 13 ] @property def bam_mapq ( self ): return self . bam [ 13 : 14 ] @property def bam_bin ( self ): return self . bam [ 14 : 16 ] @property def bam_n_cigar_op ( self ): return self . bam [ 16 : 18 ] @property def bam_flag ( self ): return self . bam [ 18 : 20 ] @property def bam_l_seq ( self ): return self . bam [ 20 : 24 ] @property def bam_next_refID ( self ): return self . bam [ 24 : 28 ] @property def bam_pnext ( self ): return self . bam [ 28 : 32 ] @property def bam_tlen ( self ): return self . bam [ 32 : 36 ] @property def bam_qname ( self ): return self . bam [ 36 : self . _end_of_qname ] @property def bam_cigar ( self ): return self . bam [ self . _end_of_qname : self . _end_of_cigar ] @property def bam_seq ( self ): return self . bam [ self . _end_of_cigar : self . _end_of_seq ] @property def bam_qual ( self ): return self . bam [ self . _end_of_seq : self . _end_of_qual ] @property def bam_tags ( self ): return self . bam [ self . _end_of_qual : ] @property def sam_refID ( self ): return unpack ( '<i' , self . bam [ 4 : 8 ] )[ 0 ] @property def sam_pos0 ( self ): return unpack ( '<i' , self . bam [ 8 : 12 ] )[ 0 ] @property def sam_l_read_name ( self ): return unpack ( '<B' , self . bam [ 12 : 13 ] )[ 0 ] @property def sam_mapq ( self ): return unpack ( '<B' , self . bam [ 13 : 14 ] )[ 0 ] @property def sam_bin ( self ): return unpack ( '<H' , self . bam [ 14 : 16 ] )[ 0 ] @property def sam_n_cigar_op ( self ): return unpack ( '<H' , self . bam [ 16 : 18 ] )[ 0 ] @property def sam_flag ( self ): return unpack ( '<H' , self . bam [ 18 : 20 ] )[ 0 ] @property def sam_l_seq ( self ): return unpack ( '<i' , self . bam [ 20 : 24 ] )[ 0 ] @property def sam_next_refID ( self ): return unpack ( '<i' , self . bam [ 24 : 28 ] )[ 0 ] @property def sam_pnext0 ( self ): return unpack ( '<i' , self . bam [ 28 : 32 ] )[ 0 ] @property def sam_tlen ( self ): return unpack ( '<i' , self . bam [ 32 : 36 ] )[ 0 ] @property def sam_qname ( self ): return self . bam [ 36 : self . _end_of_qname - 1 ] # -1 to remove trailing NUL byte @property def sam_cigar_list ( self ): return [ ( cig >> 4 , cigar_codes [ cig & 0b1111 ] ) for cig in array ( 'I' , self . bam [ self . _end_of_qname : self . _end_of_cigar ])] @property def sam_cigar_string ( self ): return '' . join ( [ str ( cig >> 4 ) + cigar_codes [ cig & 0b1111 ] for cig in array ( 'I' , self . bam [ self . _end_of_qname : self . _end_of_cigar ])]) @property def sam_seq ( self ): return '' . join ( [ dna_codes [ dna >> 4 ] + dna_codes [ dna & 0b1111 ] for dna in array ( 'B' , self . bam [ self . _end_of_cigar : self . _end_of_seq ])])[: self . sam_l_seq ] # As DNA is 4 bits packed 2-per-byte, there might be a trailing '0000', so we can either @property def sam_qual ( self ): return '' . join ( [ chr ( ord ( quality ) + 33 ) for quality in self . bam [ self . _end_of_seq : self . _end_of_qual ]]) @property def sam_tags_list ( self ): result = [] offset = self . _end_of_qual while offset != len ( self . bam ): tag_name = self . bam [ offset : offset + 2 ] tag_type = self . bam [ offset + 2 ] if tag_type == 'Z' : offset_end = self . bam . index ( ' \\x00 ' , offset + 3 ) + 1 tag_data = self . bam [ offset + 3 : offset_end - 1 ] elif tag_type in CtoPy : offset_end = offset + 3 + py4py [ tag_type ] tag_data = unpack ( CtoPy [ tag_type ], self . bam [ offset + 3 : offset_end ])[ 0 ] elif tag_type == 'B' : offset_end = offset + 8 + ( unpack ( '<i' , self . bam [ offset + 4 : offset + 8 ])[ 0 ] * py4py [ self . bam [ offset + 3 ]]) tag_data = array ( self . bam [ offset + 3 ] , self . bam [ offset + 8 : offset_end ] ) else : print ( 'PYBAM ERROR: I dont know how to parse BAM tags in this format: ' , repr ( tag_type )) print ( ' This is simply because I never saw this kind of tag during development.' ) print ( ' If you could mail the following chunk of text to john at john.uk.com, ill fix this up :)' ) print ( repr ( tag_type ), repr ( self . bam [ offset + 3 : end ])) exit () result . append (( tag_name , tag_type , tag_data )) offset = offset_end return result @property def sam_tags_string ( self ): return ' \\t ' . join ( A + ':' + ( 'i' if B in 'cCsSI' else B ) + ':' + (( C . typecode + ',' + ',' . join ( map ( str , C ))) if type ( C ) == array else str ( C )) for A , B , C in self . sam_tags_list ) ## BONUS methods - methods that mimic how samtools works. @property def sam_pos1 ( self ): return 0 if self . sam_pos0 < 0 else self . sam_pos0 + 1 @property def sam_pnext1 ( self ): return 0 if self . sam_pnext0 < 0 else self . sam_pnext0 + 1 @property def sam_rname ( self ): return '*' if self . sam_refID < 0 else self . file_chromosomes [ self . sam_refID ] @property def sam_rnext ( self ): return '*' if self . sam_next_refID < 0 else self . file_chromosomes [ self . sam_next_refID ] @property def sam ( self ): return ( self . sam_qname + ' \\t ' + str ( self . sam_flag ) + ' \\t ' + self . sam_rname + ' \\t ' + str ( self . sam_pos1 ) + ' \\t ' + str ( self . sam_mapq ) + ' \\t ' + ( '*' if self . sam_cigar_string == '' else self . sam_cigar_string ) + ' \\t ' + ( '=' if self . bam_refID == self . bam_next_refID else self . sam_rnext ) + ' \\t ' + str ( self . sam_pnext1 ) + ' \\t ' + str ( self . sam_tlen ) + ' \\t ' + self . sam_seq + ' \\t ' + self . sam_qual + ' \\t ' + self . sam_tags_string ) ## Internal methods - methods used to calculate where variable-length blocks start/end @property def _end_of_qname ( self ): return self . sam_l_read_name + 36 # fixed-length stuff at the beginning takes up 36 bytes. @property def _end_of_cigar ( self ): return self . _end_of_qname + ( 4 * self . sam_n_cigar_op ) # 4 bytes per n_cigar_op @property def _end_of_seq ( self ): return self . _end_of_cigar + ( - (( - self . sam_l_seq ) // 2 )) # {blurgh} @property def _end_of_qual ( self ): return self . _end_of_seq + self . sam_l_seq # qual has the same length as seq def __del__ ( self ): if self . _subprocess . returncode is None : self . _subprocess . kill () self . _file . close () class PybamWarn ( Exception ): pass DOCS class PybamError ( Exception ): pass DOCS","title":"ecodam_py.pybam"}]}